{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d91f35-01a2-4612-8e37-3906316f99c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Precompute phase:\n",
    "# Go through dataset\n",
    "# Grab list of mentions\n",
    "# Query LLM to get new mentions and entities\n",
    "# Save them in a good format\n",
    "\n",
    "# Generation phase:\n",
    "# Re-Generate data with a given seed\n",
    "#import unsloth\n",
    "#from unsloth import FastLanguageModel\n",
    "# need C compiler...\n",
    "\n",
    "STAGES = {0: \"load dataset\", \n",
    "          1: \"generate mentions\", \n",
    "          2: \"verify entities\", \n",
    "          3: \"generate nif\", \n",
    "          4: \"generate candidates\", \n",
    "          5: \"generate descriptions\", \n",
    "          6: \"check for missing descriptions\", \n",
    "          7: \"generate aquaint nif\", \n",
    "          -1: \"generate_aquaint_nif\"}\n",
    "# 0, 4, 5, 6 are the ones we need to run\n",
    "STAGE = STAGES[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8986814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  from datasets import load_dataset\n",
    "\n",
    "#  dataset = load_dataset(\"strombergnlp/ipm_nel\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "082ec445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Generate NIF dataset for AQUAINT\n",
    "from pynif import NIFCollection\n",
    "\n",
    "if STAGE == \"generate_aquaint_nif\":\n",
    "    base_uri = \"http://example.org/AQUAINT/\"\n",
    "    collection = NIFCollection(uri=base_uri)\n",
    "    document_uri = base_uri + \"document\"\n",
    "    nif_dataset_output_path = \"./data/AQUAINT.ttl\"\n",
    "    if STAGE == \"generate aquaint nif\":\n",
    "        dataset = load_dataset(\"boragokbakan/entity_disambiguation\", \"aquaint\")\n",
    "        test_data = dataset['test']\n",
    "        test_idx = 1\n",
    "        #print(test_data['candidates'][test_idx])\n",
    "        #print(test_data['input'][test_idx])\n",
    "        #print(test_data['id'][test_idx])\n",
    "        #print(test_data['meta'][test_idx])#'left context', 'right context', 'mention'\n",
    "        #print(test_data['answer'][test_idx])\n",
    "        for i in range(len(test_data)):\n",
    "            #entity = test_data['answer'][i]\n",
    "            document_uri = base_uri + f\"document{i}\"\n",
    "            # Add sentence (document) to context\n",
    "            sentence_str = test_data['input'][i]\n",
    "            phrases = []\n",
    "            start_marker = \"[START_ENT] \"\n",
    "            startoffset = sentence_str.index(start_marker)\n",
    "            end_marker   = \" [END_ENT]\"\n",
    "            # For each mention\n",
    "            while start_marker in sentence_str:\n",
    "                # Get the mention\n",
    "                startoffset = sentence_str.index(start_marker)\n",
    "                endoffset = sentence_str.index(end_marker, startoffset)\n",
    "                # Get the mention\n",
    "                mention = sentence_str[startoffset + len(start_marker):endoffset]\n",
    "                sentence_str = sentence_str[:startoffset] + mention + sentence_str[endoffset + len(end_marker):]\n",
    "                # Get the entity\n",
    "                entity = test_data['answer'][i]\n",
    "                #print(entity)\n",
    "                # Add an entry\n",
    "                phrases.append({\n",
    "                    \"beginIndex\": startoffset,\n",
    "                    \"endIndex\": startoffset + len(mention),\n",
    "                    \"taIdentRef\": 'http://dbpedia.org/resource/' + entity.replace(\" \", \"_\"),\n",
    "                })\n",
    "\n",
    "            context = collection.add_context(\n",
    "                uri=document_uri,\n",
    "                mention=sentence_str)\n",
    "            \n",
    "            for phrase in phrases:\n",
    "                # Add a phrase to the context\n",
    "                context.add_phrase(\n",
    "                    beginIndex=phrase['beginIndex'],\n",
    "                    endIndex=phrase['endIndex'],\n",
    "                    taIdentRef=phrase['taIdentRef']\n",
    "                )\n",
    "\n",
    "\n",
    "        generated_nif = collection.dumps(format='turtle')\n",
    "        #print(generated_nif)\n",
    "        with open(nif_dataset_output_path, \"w\", encoding='utf-8') as dataset_file:\n",
    "            dataset_file.write(generated_nif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d64f4296-a000-412e-abe7-70c2c7131123",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load NIF dataset\n",
    "import utils\n",
    "nif_data_folder_path = \"/mnt/webscistorage/wf7467/agnos/data/\"\n",
    "nif_dataset_names = [\"AIDA-Syn_mini_10_42.nif\", \"AIDA-Syn_mini_50_42.nif\", \"AIDA-Syn_mini_100_42.nif\", \"AQUAINT.ttl\", \"KORE_50_DBpedia.ttl\", \"ACE2004t.ttl\", \"MSNBCt.ttl\", \"aida_testc.ttl\", \"RSS-500.ttl\", \"AIDA-YAGO2-dataset.tsv_nif\"]\n",
    "nif_data_path_lst = [nif_data_folder_path + name for name in nif_dataset_names]\n",
    "nif_data_path_idx = 0\n",
    "# Just the name without the path\n",
    "nif_dataset_name = nif_dataset_names[nif_data_path_idx]\n",
    "nif_data_path = nif_data_path_lst[nif_data_path_idx]\n",
    "# response_file_path is within _mappings.json\n",
    "out_mappings_info_dir = nif_data_path + \"_mappings/\"\n",
    "\n",
    "# Now we need to save our descriptions somewhere\n",
    "out_descriptions_dir = nif_data_path + \"_descriptions/\"\n",
    "\n",
    "\n",
    "\n",
    "if STAGE == \"load dataset\":\n",
    "    # Load NIF dataset\n",
    "    print(\"Loading NIF dataset...\")\n",
    "    # Load the dataset\n",
    "    collection = utils.load_nif_dataset(nif_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38411d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go through all lines with specific regex criteria and output in a list\n",
    "# Only relevant for homebrew dataset\n",
    "if False:\n",
    "    # This is only if we need to generate the context list because some NIF libraries don't take care of it\n",
    "    # But PyNIF requires it to figure out what the context documents are\n",
    "    print(\"NIF path: \",nif_data_path)\n",
    "    hasContext_docEntities = utils.compute_nif_hasContext(nif_data_path)\n",
    "    print(list(hasContext_docEntities))\n",
    "    hasContext_docEntities = utils.compute_nif_hasContextKORE50(nif_data_path)\n",
    "    print(list(hasContext_docEntities))\n",
    "    hasContext_docEntities = utils.compute_nif_hasContextACE2004t(nif_data_path)\n",
    "    print(list(hasContext_docEntities))\n",
    "    hasContext_docEntities = utils.compute_nif_hasContextMSNBCt(nif_data_path)\n",
    "    print(list(hasContext_docEntities))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d57a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "if STAGE == \"load dataset\":\n",
    "    print(collection)\n",
    "    for context in collection.contexts:\n",
    "        json_options = {}\n",
    "        try:\n",
    "            mapping_document_candidate_info = {}\n",
    "\n",
    "            # See what can be called on context\n",
    "            # print(dir(context))\n",
    "\n",
    "            # document/context.mention = input sentence (1 document = 1 sentence)\n",
    "            document = context.mention\n",
    "            # If it's done already, continue to next\n",
    "\n",
    "            # Document string will be used as key for all candidate information\n",
    "            key = document\n",
    "\n",
    "            mentions = [phrase.mention for phrase in context.phrases]\n",
    "            entities = [phrase.taIdentRef.replace(\"http://dbpedia.org/resource/\", \"\").replace(\"http://en.wikipedia.org/wiki/\", \"\") for phrase in context.phrases]\n",
    "\n",
    "            json_options = {}\n",
    "            json_options['sentence'] = document\n",
    "            json_options['mentions'] = {}\n",
    "            json_options['mentions']['beginIndex'] = [phrase.beginIndex for phrase in context.phrases]\n",
    "            json_options['mentions']['endIndex'] = [phrase.endIndex for phrase in context.phrases]\n",
    "            json_options['mentions']['uris'] = entities\n",
    "            json_options['mentions']['mention'] = mentions\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "412129fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please install OpenAI SDK first: `pip3 install openai`\n",
    "from openai import OpenAI\n",
    "import json, os, glob\n",
    "import os.path\n",
    "\n",
    "# Load NIF dataset\n",
    "import utils\n",
    "\n",
    "if STAGE == \"generate mentions\":\n",
    "\n",
    "    collection = utils.load_nif_dataset(nif_data_path)\n",
    "    print(f\"Finished loading dataset[{nif_data_path}]: {collection}\")\n",
    "\n",
    "    prompt_path = \"/mnt/webscistorage/wf7467/agnos/prompt_generate_5_new_mention_minimized.txt\"\n",
    "    prompt_template = \"\"\n",
    "    # Load the prompt template from a file\n",
    "    with open(prompt_path, \"r\") as f:\n",
    "        prompt_template =  f.read()\n",
    "\n",
    "    # Load API key from file\n",
    "    api_key_file = \".deepseek_api_key.txt\"\n",
    "    with open(api_key_file, \"r\") as f:\n",
    "        DEEPSEEK_API_KEY = f.read().strip()\n",
    "\n",
    "    # Init LLM client\n",
    "    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "\n",
    "    out_dir = nif_data_path_lst[nif_data_path_idx] + \"_options/\"\n",
    "    #out_dir = \"/mnt/webscistorage/wf7467/agnos/data/AIDA-YAGO2-dataset.tsv_nif_options/\"\n",
    "\n",
    "    # Save mappings for input document to output document(s)\n",
    "    out_mappings_info_dir = nif_data_path_lst[nif_data_path_idx] + \"_mappings/\"\n",
    "\n",
    "    # Load all mappings\n",
    "    mapping_keys = set()\n",
    "    # List all files in out_mappings_info_dir\n",
    "    # Get all JSON files in the directory\n",
    "    json_mapping_files = glob.glob(os.path.join(out_mappings_info_dir, \"*.json\"))\n",
    "    for mapping_file in json_mapping_files:\n",
    "        with open(mapping_file, encoding='utf-8') as json_file:\n",
    "            in_json = json.load(json_file)\n",
    "            # Adapt to respective JSON structure\n",
    "            mapping_keys.add(in_json.get(\"original_uri\", \"\"))\n",
    "            # { \"original_uri\": str, \"response_file_path\": str, \"document\": str}\n",
    "\n",
    "\n",
    "\n",
    "    QUERY_LLM = True\n",
    "\n",
    "    file_identifier = 0\n",
    "    for context in collection.contexts:\n",
    "\n",
    "        json_options = {}\n",
    "        try:\n",
    "            # See what can be called on context\n",
    "            # print(dir(context))\n",
    "            # context_actions = [context.add_phrase, context.beginIndex, context.endIndex, context.isContextHashBasedString, context.load_from_graph, context.mention, context.original_uri, context.phrases, context.sourceUrl, context.triples, context.turtle, context.uri]\n",
    "            # context_action_names = ['add_phrase', 'beginIndex', 'endIndex', 'isContextHashBasedString', 'load_from_graph', 'mention', 'original_uri', 'phrases', 'sourceUrl', 'triples', 'turtle', 'uri']\n",
    "            # for action, name in zip(context_actions, context_action_names):\n",
    "            #     print(f\"{name}: {action}\")\n",
    "\n",
    "            # Store following info: original_uri, document, response_file_path\n",
    "            # document/context.mention = input sentence (1 document = 1 sentence)\n",
    "            document = context.mention\n",
    "            \n",
    "            # Document string will be used as key for all candidate information\n",
    "            key = context.original_uri\n",
    "\n",
    "            # If it's done already, continue to next\n",
    "            # Check mappings\n",
    "            if key in mapping_keys:\n",
    "                print(f\"Found key[{key}] Skipping...\")\n",
    "                continue\n",
    "\n",
    "            sorted_mentions_entities = sorted(\n",
    "                [(phrase.mention, phrase.beginIndex, phrase.endIndex, phrase.taIdentRef) for phrase in context.phrases],\n",
    "                key=lambda x: x[1]\n",
    "            )\n",
    "            # Only take the actual mention rather than the indices for displaying\n",
    "            mentions = [mention[0] for mention in sorted_mentions_entities]\n",
    "            # Also have the entities sorted appropriately - otherwise the indices are faulty\n",
    "            entities = [entity[3].replace(\"http://dbpedia.org/resource/\", \"\").replace(\"http://en.wikipedia.org/wiki/\", \"\") for entity in sorted_mentions_entities]\n",
    "\n",
    "            # json_options = {}\n",
    "            # json_options['sentence'] = document\n",
    "            # json_options['mentions'] = {}\n",
    "            # json_options['mentions']['beginIndex'] = [phrase.beginIndex for phrase in context.phrases]\n",
    "            # json_options['mentions']['endIndex'] = [phrase.endIndex for phrase in context.phrases]\n",
    "            # json_options['mentions']['uris'] = entities\n",
    "            # json_options['mentions']['mention'] = mentions\n",
    "\n",
    "            print(mentions)\n",
    "            print(entities)\n",
    "            print(document)\n",
    "\n",
    "            prompt = prompt_template\n",
    "            prompt += \"Original Text: \\\"\"+document + \"\\\"\\n\" #Officer Korey Lankow was placed on administrative leave after leaving Jeg , a drug-sniffing dog , in his squad car outside DPS headquarters in Tucson for more than an hour on July 11 .\"\n",
    "            prompt += \"Mentions: [\\\"\" + \"\\\", \\\"\".join(mentions) + \"\\\"]\" +\"\\n\"#[\"Tucson\", \"DPS\"]\n",
    "            prompt += \"Wikipedia IDs: [\\\"\" + \"\\\", \\\"\".join(entities) + \"\\\"]\"  + \"\\n\"#[\"Tucson,_Arizona\", \"Department_of_Public_Safety\"]\n",
    "            # Original Text: \"Samsung released a new phone in Seoul.\"  \n",
    "            # Mentions: [\"Samsung\", \"Seoul\"]  \n",
    "            # Wikipedia IDs: [\"Samsung\", \"Seoul\"]  \n",
    "            #print(prompt)\n",
    "\n",
    "            # Response output path\n",
    "            out_response_path = out_dir + str(file_identifier) + \"_response.json\"\n",
    "            while os.path.isfile(out_response_path):\n",
    "                file_identifier += 1\n",
    "                out_response_path = out_dir + str(file_identifier) + \"_response.json\"\n",
    "\n",
    "            # Mappings output path\n",
    "            out_mappings_file = out_mappings_info_dir + str(file_identifier) + \"_mappings.json\"\n",
    "            # Add to redundancy check for skip logic\n",
    "            mapping_keys.add(key)\n",
    "\n",
    "            # Save mappings for input document to output document(s)\n",
    "            # Save the mapping information\n",
    "            out_mapping_info = {}\n",
    "            # Save the original URI for the document (e.g. <https://aifb.kit.edu/conll/1025#char=0,1766>)\n",
    "            out_mapping_info[\"original_uri\"] = context.original_uri\n",
    "            # Save the response file path, so we know where to look for the response\n",
    "            out_mapping_info[\"response_file_path\"] = out_response_path\n",
    "\n",
    "            # Save the input document text\n",
    "            out_mapping_info[\"document\"] = document\n",
    "            # Save the original mentions\n",
    "            # out_mapping_info[\"mentions\"] = mentions\n",
    "            out_mapping_info[\"mention_entities\"] = sorted_mentions_entities\n",
    "            # Save the original entities\n",
    "            # out_mapping_info[\"entities\"] = entities\n",
    "            # { \"original_uri\": str, \"response_file_path\": str, \"document\": str}\n",
    "            out_mapping_info[\"prompt\"] = prompt\n",
    "\n",
    "            if QUERY_LLM:\n",
    "                # Use the LLM to get the new mentions\n",
    "                # Dump the mapping JSON to a file for easy retrieval\n",
    "                with open(out_mappings_file, \"w\") as out_file:\n",
    "                    json.dump(out_mapping_info, out_file, indent=4)\n",
    "                # Open a new connection for each document\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"deepseek-chat\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant, an expert at generating new mentions for entity linking tasks.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    stream=False,\n",
    "                    max_tokens=8000\n",
    "                )\n",
    "                # Save the response to a file\n",
    "                #print(response.choices[0].message.content)\n",
    "                with open(out_response_path, \"w\") as f:\n",
    "                    f.write(response.choices[0].message.content)\n",
    "                    print(\"Saved: \", out_response_path)\n",
    "\n",
    "            #break\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document: {e}\")\n",
    "            continue\n",
    "        file_identifier += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8687646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Senegal', 'DAKAR', 'Senegal', 'Kaolack', 'Reuters', 'Senegalese', 'Dakar']\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import sparqlutils\n",
    "import candgenutils\n",
    "import glob, os, json\n",
    "\n",
    "print(utils.get_json_from_response_file(\"/mnt/webscistorage/wf7467/agnos/data/AIDA-YAGO2-dataset.tsv_nif_options/176_response.json\")['mentions'])\n",
    "# yay, we can extract the new dataset stuff!\n",
    "# Now, what needs to be done?\n",
    "# 0. Verify that all wikipedia IDs are correct (and actually exist by doing a DBpedia lookup for description and type)\n",
    "# 1. Get the new mentions and transform them into sentences by replacing {0} etc. with appropriate position mention\n",
    "# After all have run through, transform into NIF datasets (to make downstream processing uniform)\n",
    "# 2. Generate candidates for the new mentions (DBpedia Lookup or BLINK)\n",
    "# 3. Generate types for each candidate (DBpedia Lookup? Or SPARQL query to DBpedia endpoint)\n",
    "# 4. Query descriptions (SPARQL query via DBpedia endpoint)\n",
    "\n",
    "def get_info_from_response_json(key, response_json, response_json_filepath):\n",
    "    # Get the wanted information from the response\n",
    "    info = response_json.get(key, None)\n",
    "    if info == []:\n",
    "        raise ValueError(f\"No [{key}] found in [{response_file_path}] for [{response_json}]\")\n",
    "    return info\n",
    "\n",
    "\n",
    "if STAGE == \"verify entities\":\n",
    "    prefix_dbpedia = \"http://dbpedia.org/resource/\"\n",
    "    # Load mappings\n",
    "    # Get all JSON files in the directory\n",
    "    json_mapping_files = glob.glob(os.path.join(out_mappings_info_dir, \"*.json\"))\n",
    "    for mapping_file in json_mapping_files:\n",
    "        base_filename = os.path.basename(mapping_file)\n",
    "\n",
    "        if os.path.isfile(out_descriptions_dir +\"/\"+base_filename):\n",
    "            print(f\"Skipping... Descriptions File[{base_filename}] already exists\")\n",
    "            continue\n",
    "\n",
    "        with open(mapping_file, encoding='utf-8') as json_file:\n",
    "            in_json = json.load(json_file)\n",
    "            # Adapt to respective JSON structure\n",
    "            # mapping_keys.add(in_json.get(\"original_uri\", \"\"))\n",
    "            # { \"original_uri\": str, \"response_file_path\": str, \"document\": str}\n",
    "\n",
    "            response_file_path = in_json.get(\"response_file_path\", \"\")\n",
    "            if response_file_path == \"\":\n",
    "                print(\"No response file path found\")\n",
    "                continue\n",
    "            if not os.path.isfile(response_file_path):\n",
    "                print(f\"Response file path does not exist: {response_file_path}\")\n",
    "                continue\n",
    "            # Load the response file\n",
    "            try:\n",
    "                response_json = utils.get_json_from_response_file(response_file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading JSON response file: {e}\")\n",
    "                # Output to an error log file\n",
    "                error_log_path = response_file_path.replace(\".json\", \"_error.log\")\n",
    "                with open(error_log_path, \"a\") as error_log_file:\n",
    "                    error_log_file.write(f\"Error loading JSON response file[{response_file_path}]: {e}\\n\")\n",
    "            # response_json = utils.get_json_from_response_file(response_file_path)\n",
    "            # Get the mentions from the response\n",
    "            try:\n",
    "                error_spot = \"input_text\"\n",
    "                gen_text_w_placeholders = get_info_from_response_json(\"input_text\", response_json, response_file_path)\n",
    "                error_spot = \"mentions\"\n",
    "                original_mentions = get_info_from_response_json(\"mentions\", response_json, response_file_path)\n",
    "                error_spot = \"replacements\"\n",
    "                gen_replacements = get_info_from_response_json(\"replacements\", response_json, response_file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting {error_spot} from response file: {e}\")\n",
    "                # Output to an error log file\n",
    "                error_log_path = response_file_path.replace(\".json\", \"_error.log\")\n",
    "                with open(error_log_path, \"a\") as error_log_file:\n",
    "                    error_log_file.write(f\"Error loading JSON response file[{response_file_path}]: {e}\\n\")\n",
    "\n",
    "            #entities = [[(m) for m in e] for e, key in enumerate(replacements)]\n",
    "            #print(entities)\n",
    "            mention_entities = [[(m['new'], prefix_dbpedia + m['wiki']) for m in e['entities']] for e in gen_replacements]\n",
    "            # Let's already do a query for this document and its alternatives\n",
    "            uris_to_query = list(set([entry[1] for mention_entity in mention_entities for entry in mention_entity]))\n",
    "            print(uris_to_query)\n",
    "\n",
    "            # Step of 200 would hopefully work\n",
    "            step = 200\n",
    "            file_counter_desc = 0\n",
    "\n",
    "            for i in range(0, len(uris_to_query), step):\n",
    "                print(f\"[{response_file_path}] Working on range[{i}, {i+step}]\")\n",
    "                if len(uris_to_query) > step:\n",
    "                    desc_file_name = base_filename.replace(\".json\", \"\")+\"_\"+str(i)\n",
    "                else:\n",
    "                    desc_file_name = base_filename.replace(\".json\", \"\")\n",
    "                # Query DBpedia for descriptions\n",
    "                dict_uri_desc = sparqlutils.query_descriptions_multiple_uris(uris_to_query[i:i+step])\n",
    "                candgenutils.save_cand_desc_result(cand_desc_dictionary=dict_uri_desc, file_counter=desc_file_name, result_directory=out_descriptions_dir)\n",
    "                file_counter_desc += 1\n",
    "            \n",
    "            # Check if all entities are valid and exist in DBpedia by querying information\n",
    "            # Gather all URIs\n",
    "            #entities = [entity.replace(\"http://dbpedia.org/resource/\", \"\") for entity in entities]\n",
    "            #entities = [entity.replace(\"http://en.wikipedia.org/wiki/\", \"\") for entity in entities]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95fa0a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get all mentions\n",
    "# Step 2: Then generate candidates --> candgenutils.generate_candidates(mention)\n",
    "# Step 1: Generate NIF dataset\n",
    "# Step 1.1: Available data:\n",
    "#   Entrypoint: Original mentions: AIDA-YAGO2-dataset.tsv_nif\n",
    "#   New mentions: AIDA-YAGO2-dataset.tsv_nif_options/0_response.json\n",
    "# Step 2: \n",
    "# Step 3: Generate candidates and descriptions by calling compute_save_candidates_for_mentions(result_directory, NIFCollection)\n",
    "# Generate descriptions + save files --> generate_candidates_and_descriptions\n",
    "\n",
    "\n",
    "import random, re\n",
    "from pynif import NIFCollection\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "# for each mapping file, get response_file_path\n",
    "# take .get(\"original_uri\").replace(\"https://aifb.kit.edu/conll/\", f\"https://aifb.kit.edu/conll/synthetic/{seed}/\")\n",
    "#mylist = [\"apple\", \"banana\", \"cherry\"]\n",
    "#print(random.choice(mylist))\n",
    "# \n",
    "\n",
    "nif_dataset_output_path = f\"/mnt/webscistorage/wf7467/agnos/data/\"+nif_dataset_name+\"_generated_dataset_{seed}.nif\"\n",
    "institute_uri = \"https://anonymous.com/\"\n",
    "llm_name = 'deepseek'\n",
    "original_base_uri = \"https://anonymised.com/conll/\"\n",
    "base_uri = original_base_uri + f\"synthetic/{seed}/\"\n",
    "prefix_dbpedia = \"http://dbpedia.org/resource/\"\n",
    "\n",
    "if STAGE == \"generate nif\":\n",
    "\n",
    "    collection = NIFCollection(uri=base_uri)\n",
    "\n",
    "\n",
    "    def replace_mention_at_index(input_text, mention, replacement, begin_index):\n",
    "        # Ensure the mention exists at the specified begin_index\n",
    "        if input_text[begin_index:begin_index + len(mention)] == mention:\n",
    "            # Replace the mention at the specified index\n",
    "            input_text = (\n",
    "                input_text[:begin_index] + replacement + input_text[begin_index + len(mention):]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Mention '{mention}' (instead '{input_text[begin_index:begin_index + len(mention)]}') not found at index {begin_index} in the input text.\")\n",
    "        return input_text\n",
    "\n",
    "    def compute_new_start_indices(desc_order_mentions_original):\n",
    "        # Compute the new start indices for the mentions in the modified text\n",
    "        #print(\"desc_order_mentions_original: \", desc_order_mentions_original)\n",
    "        new_start_indices = []\n",
    "        for i, tpl in enumerate(desc_order_mentions_original):\n",
    "            begin_index = tpl[1]\n",
    "            modifier = 0\n",
    "            for j in range(i+1, len(desc_order_mentions_original)):\n",
    "                prior_tpl = desc_order_mentions_original[j]\n",
    "                # Get the replacement mention\n",
    "                replacement = prior_tpl[2]\n",
    "                # Get the old mention\n",
    "                old_mention = prior_tpl[0]\n",
    "                # Compute the new start index\n",
    "                modifier += len(replacement) - len(old_mention)\n",
    "            new_start_index = begin_index + modifier\n",
    "            new_start_indices.append(new_start_index)\n",
    "        #print(f\"New start indices: {new_start_indices}\")\n",
    "        return new_start_indices\n",
    "\n",
    "    def inject_mentions_into_original_text(original_document, original_mentions, original_beginIndices, original_endIndices, original_entities, mention_entities, gen_text_w_placeholders):\n",
    "\n",
    "        #original_mentions = [mention_entity[0] for mention_entity in original_mention_entities]\n",
    "        #original_beginIndices = [mention_entity[1] for mention_entity in original_mention_entities]\n",
    "        #original_endIndices = [mention_entity[2] for mention_entity in original_mention_entities]\n",
    "        #original_entities = [mention_entity[3].replace(\"http://dbpedia.org/resource/\", \"\").replace(\"http://en.wikipedia.org/wiki/\", \"\") for mention_entity in original_mention_entities]\n",
    "\n",
    "        new_mentions = [mention_entity['new'] for mention_entity in mention_entities]\n",
    "        new_entities = [mention_entity['wiki'] for mention_entity in mention_entities]\n",
    "\n",
    "        if len(original_mentions) == len(mention_entities):\n",
    "            # same number of mentions in original and generated text\n",
    "            # We can just replace the mentions with the generated ones\n",
    "\n",
    "            # Iterate from last index to first index and replace the mentions\n",
    "            desc_order_mentions_original = [\n",
    "                (mention, beginIndex, new_mention, original_entity, new_entity) for beginIndex, mention, new_mention, original_entity, new_entity in sorted(\n",
    "                    zip(original_beginIndices, original_mentions, new_mentions, original_entities, new_entities),\n",
    "                    key=lambda x: x[0],  # Sort by original_beginIndices\n",
    "                    reverse=True         # Descending order\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            modified_text = original_document\n",
    "            for i, tpl in enumerate(desc_order_mentions_original):\n",
    "                mention = tpl[0]\n",
    "                begin_index = tpl[1]\n",
    "                # Get the replacement mention\n",
    "                replacement = tpl[2]\n",
    "                print(\"Tuple: \",tpl)\n",
    "                try:\n",
    "                    # Replaces only if mention is exactly there at the right start index\n",
    "                    modified_text = replace_mention_at_index(modified_text, mention, replacement, begin_index)\n",
    "                    # How can we compute the new begin index once we have gone through all of them?\n",
    "\n",
    "                except ValueError as e:\n",
    "                    raise e\n",
    "                    # Could not find string at the right spot, let's see if we can find it somewhere though, it might be a faulty index\n",
    "                    new_begin_index = modified_text.rindex(mention)\n",
    "                    # Attempt it again - if this doesn't work, then it shouldn't work\n",
    "                    modified_text = replace_mention_at_index(modified_text, mention, replacement, new_begin_index)\n",
    "\n",
    "            new_start_indices = compute_new_start_indices(desc_order_mentions_original)\n",
    "\n",
    "            # # Verifying if new index computation works:\n",
    "            # for i, idx in enumerate(new_start_indices):\n",
    "            #     new_start_index = idx\n",
    "            #     # Get the replacement mention\n",
    "            #     replacement = desc_order_mentions_original[i][2]\n",
    "            #     # Get the old mention\n",
    "            #     old_mention = desc_order_mentions_original[i][0]\n",
    "            #     # Compute the new start index\n",
    "            #     print(f\"New start index: {new_start_index} for {replacement} (old: {old_mention}) - Verification: {modified_text[new_start_index: new_start_index + len(replacement)]}\")\n",
    "\n",
    "            # Now actually inject the new indices into the data structure\n",
    "            desc_order_mentions_updated = [(tpl[0], new_start_indices[i], tpl[2], tpl[3], tpl[4]) for i, tpl in enumerate(desc_order_mentions_original)]\n",
    "\n",
    "            # sort it in ascending order by begin index again\n",
    "            asc_order_mentions_updated = sorted(\n",
    "                desc_order_mentions_updated,\n",
    "                key=lambda x: x[1]  # Sort by new start indices\n",
    "            )\n",
    "            # returns the new text as well as the old mentions, start indices, new mentions\n",
    "            # asc_order_mentions_updated: (mention, beginIndex, new_mention, original_entity, new_entity)\n",
    "            return modified_text, asc_order_mentions_updated\n",
    "\n",
    "\n",
    "        elif len(original_mentions) < len(mention_entities):\n",
    "            # FIXABLE\n",
    "            # Modified text has more mentions than original text\n",
    "            # --> Remove them from the modified text\n",
    "\n",
    "            # Identify which ones correspond to the original mentions\n",
    "            # and which ones are hallucinated\n",
    "            # hallucinated mentions should also be replaced but not listed as \"mentions\"\n",
    "\n",
    "            # Let's check if the generated mentions are in the original text\n",
    "            iter_matches = re.finditer(r\"\\{\\d{1,3}\\}\", gen_text_w_placeholders)\n",
    "            patched_text = \"\"\n",
    "            prev_index = 0\n",
    "            for i, match in enumerate(iter_matches):\n",
    "                patched_text += gen_text_w_placeholders[prev_index : match.start()]\n",
    "                patched_text += new_mentions[i]\n",
    "                prev_index = match.end()\n",
    "                #: match.end()]\n",
    "            print(\"Patched up text:\", patched_text)\n",
    "\n",
    "            #return None, None\n",
    "\n",
    "            raise ValueError(f\"More generated than wanted mentions, orig:{len(original_mentions)} vs. gen:{len(mention_entities)}: Modified text does not represent the original text well: {gen_text_w_placeholders}\")\n",
    "        elif len(original_mentions) > len(mention_entities):\n",
    "            raise ValueError(f\"[SADNESS] Gen. text does not have enough mentions: modified:({gen_text_w_placeholders}), original:({original_document})\")\n",
    "\n",
    "\n",
    "    def fix_entity(entity):\n",
    "        # Fix the entity by removing the prefix\n",
    "        entity = entity.replace(\"http://dbpedia.org/resource/\", \"\").replace(\"http://en.wikipedia.org/wiki/\", \"\")\n",
    "        entity = entity.replace('Toys_\"R\"_Us', 'Toys_R_Us')\n",
    "        return entity\n",
    "\n",
    "\n",
    "    # Load mappings\n",
    "    # Get all JSON files in the directory\n",
    "    json_mapping_files = glob.glob(os.path.join(out_mappings_info_dir, \"*.json\"))\n",
    "    skip_counter = 0\n",
    "\n",
    "    for mapping_file in json_mapping_files:\n",
    "        base_filename = os.path.basename(mapping_file)\n",
    "        with open(mapping_file, encoding='utf-8') as json_file:\n",
    "            in_json = json.load(json_file)\n",
    "            # Adapt to respective JSON structure\n",
    "            # mapping_keys.add(in_json.get(\"original_uri\", \"\"))\n",
    "            # { \"original_uri\": str, \"response_file_path\": str, \"document\": str}\n",
    "            response_file_path = in_json.get(\"response_file_path\", \"\")\n",
    "            # We need a URI or else it shouldn't work\n",
    "            original_uri = in_json.get(\"original_uri\", \"\")\n",
    "\n",
    "            # Original document\n",
    "            original_document = in_json.get(\"document\", \"\")\n",
    "\n",
    "            original_mention_entities = in_json.get(\"mention_entities\", \"\")\n",
    "\n",
    "            if original_uri == \"\":\n",
    "                print(\"No original URI found\")\n",
    "                continue\n",
    "            if response_file_path == \"\":\n",
    "                print(\"No response file path found\")\n",
    "                continue\n",
    "            if not os.path.isfile(response_file_path):\n",
    "                print(f\"Response file path does not exist: {response_file_path}\")\n",
    "                continue\n",
    "            # Load the response file\n",
    "            try:\n",
    "                response_json = utils.get_json_from_response_file(response_file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading JSON response file: {e}\")\n",
    "                # Output to an error log file\n",
    "                error_log_path = response_file_path.replace(\".json\", \"_error.log\")\n",
    "                with open(error_log_path, \"a\") as error_log_file:\n",
    "                    error_log_file.write(f\"Error loading JSON response file[{response_file_path}]: {e}\\n\")\n",
    "\n",
    "            # Get the mentions from the response\n",
    "            try:\n",
    "                error_spot = \"input_text\"\n",
    "                gen_text_w_placeholders = get_info_from_response_json(\"input_text\", response_json, response_file_path)\n",
    "                #error_spot = \"mentions\"\n",
    "                #gen_original_mentions = get_info_from_response_json(\"mentions\", response_json, response_file_path)\n",
    "                error_spot = \"replacements\"\n",
    "                gen_replacements = get_info_from_response_json(\"replacements\", response_json, response_file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting {error_spot} from response file: {e}\")\n",
    "                # Output to an error log file\n",
    "                error_log_path = response_file_path.replace(\".json\", \"_error.log\")\n",
    "                with open(error_log_path, \"a\") as error_log_file:\n",
    "                    error_log_file.write(f\"Error loading JSON response file[{response_file_path}]: {e}\\n\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            ########## Generated information #################\n",
    "            # Choose a random set of entities as replacements\n",
    "            replacement_entities = random.choice(gen_replacements)\n",
    "            mention_entities = replacement_entities.get(\"entities\")\n",
    "\n",
    "            ########## Original information #################\n",
    "            # Sort the original mentions and entities\n",
    "            original_mentions_entities_sorted = sorted(\n",
    "                [(mention_entity[0], mention_entity[1], mention_entity[2], mention_entity[3]) for mention_entity in original_mention_entities],\n",
    "                key=lambda x: x[1]\n",
    "            )\n",
    "\n",
    "            original_mentions =     [mention_entity[0] for mention_entity in original_mentions_entities_sorted]\n",
    "            original_beginIndices = [mention_entity[1] for mention_entity in original_mentions_entities_sorted]\n",
    "            original_endIndices =   [mention_entity[2] for mention_entity in original_mentions_entities_sorted]\n",
    "            original_entities =     [mention_entity[3].replace(\"http://dbpedia.org/resource/\", \"\").replace(\"http://en.wikipedia.org/wiki/\", \"\") for mention_entity in original_mentions_entities_sorted]\n",
    "\n",
    "\n",
    "            try:\n",
    "                generated_text, ordered_updated_mention_beginIndex_entities = inject_mentions_into_original_text(original_document, original_mentions, original_beginIndices, original_endIndices, original_entities, mention_entities, gen_text_w_placeholders)\n",
    "            except Exception as e:\n",
    "                skip_counter += 1\n",
    "                print(\"Skip Counter: \", skip_counter)\n",
    "                print(f\"Error injecting mentions into original text: {e}\")\n",
    "                continue\n",
    "            print(\"[ORIG TEXT]\", original_document)\n",
    "            print(f\"[GEN TEXT] {generated_text}\")\n",
    "\n",
    "            # if len(original_mentions) == len(mention_entities):\n",
    "            #     if all_indices_found:\n",
    "            #         # Found all indices in generated text (for the generated mentions) \n",
    "            #         # AND the mention counts match up with the original mention count\n",
    "\n",
    "            #         # There is consistency in the number of generated mentions and the placeholders in the generated placeholder text\n",
    "            #         # so we can technically just replace the mentions with the generated ones\n",
    "\n",
    "            #         # Replace placeholders from left to right and get the right begin indices and end indices (=begin index + mention length)\n",
    "            #         # Assume \"everything is fine\" in the generated text and mentions (incl. mention order)\n",
    "            #         beginIndices = []\n",
    "            #         endIndices = []\n",
    "            #         new_mentions = []\n",
    "            #         new_entities = []\n",
    "\n",
    "            #         # Assumption: mention_entities is sorted from left to right in order of occurence of {0}, {1}, {2} etc.\n",
    "            #         # Otherwise index calculation is faulty (e.g. in the case of {5}, {4}, {6})\n",
    "            #         for i, e in enumerate(mention_entities):\n",
    "            #             to_replace = \"{\"+str(i)+\"}\"\n",
    "            #             new_mention = e['new']\n",
    "            #             new_entity = e['wiki']\n",
    "            #             if to_replace in input_text:\n",
    "            #                 # Text contains {i} that we want to replace...\n",
    "            #                 start_index = input_text.index(to_replace)\n",
    "            #                 end_index = start_index + len(new_mention)\n",
    "            #                 input_text = input_text.replace(to_replace, new_mention, 1)\n",
    "            #                 beginIndices.append(start_index)\n",
    "            #                 endIndices.append(end_index)\n",
    "            #                 new_mentions.append(new_mention)\n",
    "            #                 new_entities.append(new_entity)\n",
    "            #             else:\n",
    "            #                 # Text does not contain {i} that we want to replace with actual\n",
    "            #                 raise ValueError(f\"[Replacement/Index retrieval] {to_replace} not found (M[{new_mention}]: E[{new_entity}]) in input text: {input_text}\")\n",
    "            #     else:\n",
    "            #         # {i} is not consistent in the generated text, so we have to do the replacement manually\n",
    "            #         # Same number of mentions though, so that's not an issue\n",
    "            #         # It's not consistent, meaning we have to rely on the original text document\n",
    "            #         generated_text = inject_mentions_into_original_text(original_document, original_mentions, original_beginIndices, original_endIndices, original_entities, mention_entities)\n",
    "\n",
    "            # else:\n",
    "            #     if len(original_mentions) < len(mention_entities):\n",
    "            #         # FIXABLE\n",
    "            #         # Modified text has more mentions than original text\n",
    "            #         # --> Remove them from the modified text\n",
    "            #         generated_text = inject_mentions_into_original_text(original_document, original_mentions, original_beginIndices, original_endIndices, original_entities, mention_entities)\n",
    "\n",
    "            #     elif len(original_mentions) > len(mention_entities):\n",
    "            #         # Modified text does not have enough mentions \n",
    "            #         # --> needs manual proofreading\n",
    "            #         raise ValueError(f\"Synthetic text does not have enough mentions: synthetic:({len(mention_entities)}), original:({len(original_mentions)})\")\n",
    "\n",
    "\n",
    "\n",
    "            # sorted_replacement_indices = sorted(replacement_indices)\n",
    "            # # Check if the replacement indices are sorted - otherwise our indexing logic is faulty\n",
    "            # if replacement_indices != sorted_replacement_indices:\n",
    "            #     # Assumption: mention_entities is sorted from left to right in order of occurence of {0}, {1}, {2} etc.\n",
    "            #     # Otherwise index calculation is faulty (e.g. in the case of {5}, {4}, {6})\n",
    "            #     raise ValueError(f\"Replacement indices are not sorted: {replacement_indices} in input text: {gen_text_w_placeholders}\")\n",
    "\n",
    "            # Finished loading everything we need, so let's create the NIF documents now\n",
    "            # Generate a document URI incl. seed etc.\n",
    "            document_uri = original_uri.replace(original_base_uri, base_uri)\n",
    "            \n",
    "            sentence_str = generated_text\n",
    "\n",
    "            # Check if the sentence actually contains all the mentions at the right spots now\n",
    "            for i, m in enumerate(ordered_updated_mention_beginIndex_entities):\n",
    "                # asc_order_mentions_updated: (mention, beginIndex, new_mention, original_entity, new_entity)\n",
    "                beginIndex = m[1]\n",
    "                new_mention = m[2]\n",
    "                endIndex = beginIndex + len(new_mention)\n",
    "                substring_mention = sentence_str[beginIndex:endIndex]\n",
    "                if substring_mention != new_mention:\n",
    "                    raise ValueError(f\"New mention '{new_mention}' vs. '{substring_mention}' not found  at [{beginIndex}, {endIndex}] in sentence:{sentence_str}\")\n",
    "\n",
    "            # Add sentence (document) to context\n",
    "            context = collection.add_context(\n",
    "            uri=document_uri,\n",
    "            mention=sentence_str)\n",
    "\n",
    "            # For each mention\n",
    "            for i, e in enumerate(ordered_updated_mention_beginIndex_entities):\n",
    "                # asc_order_mentions_updated: (mention, beginIndex, new_mention, original_entity, new_entity)\n",
    "                entity = fix_entity(e[4])\n",
    "                # Add an entry\n",
    "                context.add_phrase(\n",
    "                beginIndex=e[1],\n",
    "                endIndex=e[1]+len(e[2]),\n",
    "                #taClassRef=['http://dbpedia.org/ontology/SportsManager', 'http://dbpedia.org/ontology/Person', 'http://nerd.eurecom.fr/ontology#Person'],\n",
    "                #score=0.9869992701528016,\n",
    "                annotator=institute_uri + \"synthetic/\" + llm_name,\n",
    "                taIdentRef='http://dbpedia.org/resource/' + entity,\n",
    "                #taMsClassRef='http://dbpedia.org/ontology/SoccerManager'\n",
    "                )\n",
    "                print(\"Adding to context: \", e)\n",
    "        print(f\"Finished processing file: {mapping_file}: Skipped[{skip_counter}]\")\n",
    "            #mention_entities = [[(m['new'], prefix_dbpedia + m['wiki']) for m in e['entities']] for e in replacements]\n",
    "\n",
    "    generated_nif = collection.dumps(format='turtle')\n",
    "    with open(nif_dataset_output_path, \"w\", encoding='utf-8') as dataset_file:\n",
    "        dataset_file.write(generated_nif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e69ea64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NIF dataset from:  /mnt/webscistorage/wf7467/agnos/data/AIDA-Syn_mini_10_42.nif\n",
      "Finished reading NIF data from file(/mnt/webscistorage/wf7467/agnos/data/AIDA-Syn_mini_10_42.nif).\n",
      "<NIFCollection http://anon.ymous/>\n",
      "Generating candidates for new mentions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c433f2e3bb1484ebab89405b892091c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06835f605654c9ca959ff447554348e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not retrieve candidates for mention[Brazilian], code: 503",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m syn_collection = utils.load_nif_dataset(in_collection_path)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerating candidates for new mentions...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mcandgenutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_save_candidates_for_mentions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_candidates_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m=\u001b[49m\u001b[43msyn_collection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinished generating candidates for new mentions.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agnos/candgenutils.py:216\u001b[39m, in \u001b[36mcompute_save_candidates_for_mentions\u001b[39m\u001b[34m(result_directory, collection)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m#print(f\"{context.mention}, {context.beginIndex}, {context.endIndex}\")\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# 1 phrase = 1 mention\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m tqdm_notebook(context.phrases, position=\u001b[32m1\u001b[39m):\n\u001b[32m    213\u001b[39m     \u001b[38;5;66;03m#print(phrase)\u001b[39;00m\n\u001b[32m    214\u001b[39m     \u001b[38;5;66;03m#print(dir(phrase))\u001b[39;00m\n\u001b[32m    215\u001b[39m     \u001b[38;5;66;03m#print(f\"{phrase.mention}, {phrase.beginIndex}, {phrase.endIndex}, {phrase.generated_uri}, {phrase.taClassRef}, {phrase.taIdentRef}\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     lst_candidate_info = \u001b[43mgenerate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmention\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     labels = lst_candidate_info[\u001b[32m0\u001b[39m]\n\u001b[32m    218\u001b[39m     desc = lst_candidate_info[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agnos/candgenutils.py:25\u001b[39m, in \u001b[36mgenerate_candidates\u001b[39m\u001b[34m(mention)\u001b[39m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [[], [], [], [], [], []]\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not retrieve candidates for mention[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmention\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Response to JSON\u001b[39;00m\n\u001b[32m     27\u001b[39m json_candidates = json.loads(response.text)\n",
      "\u001b[31mValueError\u001b[39m: Could not retrieve candidates for mention[Brazilian], code: 503"
     ]
    }
   ],
   "source": [
    "# Move this to a python script...\n",
    "import candgenutils\n",
    "import utils\n",
    "\n",
    "# Generate candidates for the new mentions\n",
    "\n",
    "out_candidates_dir = \"./data/\"+nif_dataset_name+\"_candidates/\"#\"C:/Users/wf7467/Desktop/GitHub/KIT/agnostic-disambiguation/data/candidates/CoNLL_AIDA-YAGO2-dataset.nif/\"\n",
    "in_collection_path = nif_data_path#nif_dataset_output_path#\"./data/\"+\"synthetic_aida_dataset_42.nif\"\n",
    "if STAGE == \"generate candidates\":\n",
    "    print(\"Loading NIF dataset from: \", in_collection_path)\n",
    "    syn_collection = utils.load_nif_dataset(in_collection_path)\n",
    "\n",
    "    print(\"Generating candidates for new mentions...\")\n",
    "    candgenutils.compute_save_candidates_for_mentions(result_directory=out_candidates_dir, collection=syn_collection)\n",
    "    print(\"Finished generating candidates for new mentions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9907c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello worldasd\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello worldasd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# All the entities that are not in description files are not correct entities\n",
    "# -> see how many documents such things affect\n",
    "#  \n",
    "\n",
    "\n",
    "# Generate candidates for the new mentions via DBpedia Lookup or BLINK\n",
    "# Generate types for each candidate via DBpedia Lookup or SPARQL query to DBpedia endpoint\n",
    "import sparqlutils\n",
    "import candgenutils\n",
    "\n",
    "\n",
    "base_path = nif_data_folder_path\n",
    "out_candidates_dir = base_path + nif_dataset_name + \"_candidates/\"#\"C:/Users/wf7467/Desktop/GitHub/KIT/agnostic-disambiguation/data/candidates/CoNLL_AIDA-YAGO2-dataset.nif/\"\n",
    "out_candidates_descriptions_dir = base_path + nif_dataset_name + \"_candidate_descriptions/\"\n",
    "\n",
    "if STAGE in [\"check for missing descriptions\", \"generate descriptions\"]:\n",
    "    # Load existing ones\n",
    "    all_candidate_results = candgenutils.load_candidate_results(result_directory=out_candidates_dir)\n",
    "    print(f\"Loaded {len(all_candidate_results)} document entries\")\n",
    "\n",
    "\n",
    "    # For the rest: generate candidates\n",
    "\n",
    "    # save_candidate_result\n",
    "    # transform_dbp_json_cands_to_dict: DBpedia JSON to dict\n",
    "    # Step 0: Generate candidates for mentions\n",
    "    # Step 1: Collect all relevant URIs \n",
    "    # Step 2: Query descriptions for all URIs\n",
    "    # Step 3: Save the results\n",
    "\n",
    "    # Step 1: For which URIs do we need additional info (aka. descriptions)?\n",
    "    wanted_keys = ['candidate_uris', 'true_uris']\n",
    "    lst_len_counter = 0\n",
    "    uris_for_descriptions = set()\n",
    "\n",
    "    for in_file in all_candidate_results:\n",
    "        entry = all_candidate_results[in_file]\n",
    "        #print(in_file, entry.keys())\n",
    "\n",
    "        for document in entry:\n",
    "            document_candidate_json = entry[document]\n",
    "            #print(f\"Document cand. json: {document_candidate_json}\")\n",
    "            for key in wanted_keys:\n",
    "                #print(f\"Key: {key}\")\n",
    "                uris_of_uris = document_candidate_json[key]\n",
    "                #print(f\"URIs of URIs: {type(uris_of_uris[0])}\")\n",
    "                # For some reason URIS_of_URIS is sometimes a list of strings rather than list of lists - should debug\n",
    "                if uris_of_uris == []:\n",
    "                    print(f\"Empty list for [{key}] in [{document}]\")\n",
    "                    continue\n",
    "                if isinstance(uris_of_uris[0], str):\n",
    "                    # It's only one entry, so add it.\n",
    "                    uris_for_descriptions.update(uris_of_uris)\n",
    "                elif isinstance(uris_of_uris[0], list):\n",
    "                    for uris in uris_of_uris:\n",
    "                        lst_len_counter = lst_len_counter + len(uris)\n",
    "                        uris_for_descriptions.update(uris)\n",
    "                        if \")\" in uris:\n",
    "                            print(f\"Found 'h' among {len(uris)} URIs in [{key}] for [{document}]: {uris}\")\n",
    "                    #print(len(uris_for_descriptions))\n",
    "                    #print(lst_len_counter)\n",
    "            #break\n",
    "        #break\n",
    "\n",
    "    print(lst_len_counter)\n",
    "    print(len(uris_for_descriptions))\n",
    "    print(next(iter(uris_for_descriptions)))\n",
    "    uris_for_descriptions = list(uris_for_descriptions)\n",
    "\n",
    "\n",
    "if STAGE == \"generate descriptions\":\n",
    "    # Step 2: Query descriptions for all URIs & save results\n",
    "    step = 100\n",
    "    file_counter_desc = 0\n",
    "    for i in range(0, len(uris_for_descriptions), step):\n",
    "        print(\"Working on range[%d, %d]\" % (i, i+step))\n",
    "        try:\n",
    "            dict_uri_desc = sparqlutils.query_descriptions_multiple_uris(uris_for_descriptions[i:i+step])\n",
    "            # Step 3: Save results\n",
    "            candgenutils.save_cand_desc_result(cand_desc_dictionary=dict_uri_desc, file_counter=i, result_directory=out_candidates_descriptions_dir)\n",
    "            file_counter_desc += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying descriptions: {uris_for_descriptions[i:i+step]}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if STAGE == \"check for missing descriptions\":\n",
    "        \n",
    "    # Repeatedly check that everything is where it is supposed to be\n",
    "    # Checking 3 times for missing URIs... if they continue missing, it means they don't have an abstract. Most are due to being disambiguation pages\n",
    "    # Example of missing URIs:\n",
    "    # ['http://dbpedia.org/resource/Paul_Whelan', 'http://dbpedia.org/resource/Rovereto_(disambiguation)', 'http://dbpedia.org/resource/Irwin', 'http://dbpedia.org/resource/CEFCU', 'http://dbpedia.org/resource/Angolan_swallow', 'http://dbpedia.org/resource/Mountain_View,_New_Mexico', 'http://dbpedia.org/resource/Siege_of_Damascus', 'http://dbpedia.org/resource/Jerry_Simmons', 'http://dbpedia.org/resource/Frank_Richter', 'http://dbpedia.org/resource/CGU', 'http://dbpedia.org/resource/El_Gran_campeón', 'http://dbpedia.org/resource/Tsentaroy', 'http://dbpedia.org/resource/DoÃ±a_Ana_County,_New_Mexico', 'http://dbpedia.org/resource/Bethlehem_High_School', 'http://dbpedia.org/resource/List_of_unnumbered_minor_planets:_2003_A–R', 'http://dbpedia.org/resource/SPAP', 'http://dbpedia.org/resource/Charleston_Civic_Center', 'http://dbpedia.org/resource/ASAC', 'http://dbpedia.org/resource/Robert_Irwin', 'http://dbpedia.org/resource/Maarit_LepomÃ¤ki', 'http://dbpedia.org/resource/PloieÈ™ti', 'http://dbpedia.org/resource/Quattro_Canti,_Catania', 'http://dbpedia.org/resource/Peruvian_Brazilians', \"http://dbpedia.org/resource/Church_of_Sant'Angelo,_Perugia\", 'http://dbpedia.org/resource/Yelena_Vyalbe', 'http://dbpedia.org/resource/Saint-cyriens', 'http://dbpedia.org/resource/1994_Milan_Indoor_–_Singles', 'http://dbpedia.org/resource/Tuqtu', 'http://dbpedia.org/resource/Military_of_Mauritius', 'http://dbpedia.org/resource/Alistar_Fredericks', \"http://dbpedia.org/resource/2002_Kroger_St._Jude_International_–_Women's_Doubles\", 'http://dbpedia.org/resource/Wayne_Simmons', 'http://dbpedia.org/resource/Lindsay_(name)', 'http://dbpedia.org/resource/List_of_freshman_class_members_of_the_111th_United_States_Congress', 'http://dbpedia.org/resource/Neohumanism', 'http://dbpedia.org/resource/Kenan_IÅŸÄ±k', 'http://dbpedia.org/resource/Belfast_Duncairn', 'http://dbpedia.org/resource/Hum_Sab_Chor_Hain', 'http://dbpedia.org/resource/Flamengo_(disambiguation)', 'http://dbpedia.org/resource/Hà_Tĩnh_Province', 'http://dbpedia.org/resource/List_of_KDE_applications', 'http://dbpedia.org/resource/Sandusky_(surname)', 'http://dbpedia.org/resource/Kishibe', 'http://dbpedia.org/resource/Toyota_concept_vehicles_(1970–1979)', 'http://dbpedia.org/resource/FranÃ§ois_PÃ©ron', 'http://dbpedia.org/resource/WGCL-TV', \"http://dbpedia.org/resource/2017_Ricoh_Open_–_Women's_Doubles\", 'http://dbpedia.org/resource/KB_Financial_Group_Inc', 'http://dbpedia.org/resource/Arjun_Singh_(politician,_born_1962)', 'http://dbpedia.org/resource/103rd_Infantry_Division_Piacenza']\n",
    "    import candgenutils\n",
    "\n",
    "    # Check for missing descriptions\n",
    "    candgenutils.check_for_missing_uri_descriptions_and_complete(uris_for_descriptions=uris_for_descriptions, step=47, prefix=\"missing_l_\", result_directory=out_candidates_descriptions_dir)\n",
    "    candgenutils.check_for_missing_uri_descriptions_and_complete(uris_for_descriptions=uris_for_descriptions, step=23, prefix=\"missing_m_\", result_directory=out_candidates_descriptions_dir)\n",
    "    candgenutils.check_for_missing_uri_descriptions_and_complete(uris_for_descriptions=uris_for_descriptions, step=13, prefix=\"missing_n_\", result_directory=out_candidates_descriptions_dir)\n",
    "    candgenutils.check_for_missing_uri_descriptions_and_complete(uris_for_descriptions=uris_for_descriptions, step=3, prefix=\"missing_o_\", result_directory=out_candidates_descriptions_dir)\n",
    "    candgenutils.check_for_missing_uri_descriptions_and_complete(uris_for_descriptions=uris_for_descriptions, step=1, prefix=\"missing_p_\", result_directory=out_candidates_descriptions_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c457d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disamb_llm",
   "language": "python",
   "name": "disamb_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
