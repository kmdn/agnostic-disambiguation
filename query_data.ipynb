{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install jinja2 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pynif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Steps:\n",
    "\n",
    "    ## Define gold standard MD:\n",
    "\n",
    "        ### Input:  dataset\n",
    "\n",
    "        ### Output: mentions\n",
    "\n",
    "    * Generate candidates for each (DBpediaLookupFinder)\n",
    "\n",
    "    * Query DBp\n",
    "\n",
    "    * Generate prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mention data\n",
    "from pynif import NIFCollection\n",
    "\n",
    "nif_data_path = \"C:\\\\Users\\\\wf7467\\\\Desktop\\\\Evaluation Datasets\\\\Datasets\\\\entity_linking\\\\conll_aida-yago2-dataset\\\\AIDA-YAGO2-dataset.tsv_nif\"\n",
    "\n",
    "nif_data = \"\"\n",
    "\n",
    "with open(nif_data_path, 'r', encoding=\"utf-8\") as f:\n",
    "    nif_data = f.read()\n",
    "\n",
    "parsed_collection = NIFCollection.loads(nif_data, format='turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stork H1 results breakdown per sector .  AMSTERDAM 1996-08-28  First 24 weeks 1996  ( millions of guilders unless otherwise stated )  Industrial systems and components  - Turnover 756 vs 829  - Operating profit 46 vs 48  - New orders received 876 vs 933  - Order book ( billions ) 1.07 vs 0.98  Industrial services  - Turnover 657 vs 700  - Operating profit 9 vs 3  - New orders received ( billions ) 1.00 vs 1.09  - Order book ( billions ) 2.37 vs 2.01  NOTE - Order book figures refer to value of orders on books at end of period .  -- Amsterdam newsroom +31 20 504 5000 , Fax +31 20 504 5040 , 0, 595\n",
      "AMSTERDAM, 41, 50, https://aifb.kit.edu/conll/768#offset_41_50, None, http://en.wikipedia.org/wiki/Amsterdam\n",
      "Amsterdam, 538, 547, https://aifb.kit.edu/conll/768#offset_538_547, None, http://en.wikipedia.org/wiki/Amsterdam\n"
     ]
    }
   ],
   "source": [
    "for context in parsed_collection.contexts:\n",
    "    # See what can be called on context\n",
    "    # print(dir(context))\n",
    "\n",
    "    # input sentence (1 document = 1 sentence)\n",
    "    document = context.mention\n",
    "    print(f\"{context.mention}, {context.beginIndex}, {context.endIndex}\")\n",
    "    for phrase in context.phrases:\n",
    "        #print(phrase)\n",
    "        #print(dir(phrase))\n",
    "        print(f\"{phrase.mention}, {phrase.beginIndex}, {phrase.endIndex}, {phrase.generated_uri}, {phrase.taClassRef}, {phrase.taIdentRef}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "endpoint_url = \"https://lookup.dbpedia.org/api/search?format=JSON&query=\"\n",
    "\n",
    "response = requests.get(endpoint_url+\"Steve\")\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_candidates = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['score', 'refCount', 'resource', 'redirectlabel', 'typeName', 'comment', 'label', 'id', 'type', 'category'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"(['MusicalArtist', 'Person', 'Artist', 'Agent'], ['<B>Steve</B> Roach (musician)'], ['<B>Steve</B> Roach (born February 16, 1955) is an American composer and performer of ambient and'])\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Playing around with the JSON\n",
    "i = 10\n",
    "\n",
    "json_documents = json_candidates['docs']\n",
    "document = json_documents[i]\n",
    "print(document.keys())\n",
    "uri = document['resource']\n",
    "small_desc = document['comment']\n",
    "types = document['typeName']\n",
    "label = document['label']\n",
    "#json_candidates['docs'][i].keys()\n",
    "f\"{types, label, small_desc}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dbp_json_cands_to_dict(json_documents={}):\n",
    "    candidate_dict = {}\n",
    "    candidate_dict['types'] = []\n",
    "    candidate_dict['label'] = []\n",
    "    candidate_dict['desc'] = []\n",
    "    candidate_dict['uri'] = []\n",
    "    candidate_dict['score'] = []\n",
    "    candidate_dict['refCount'] = []\n",
    "    \n",
    "    for i in range(len(json_documents)):\n",
    "        document = json_documents[i]\n",
    "        try:\n",
    "            # Retrieve information\n",
    "            # Description\n",
    "            small_desc = document.get('comment', \"\")\n",
    "            # Types for this candidate\n",
    "            types = document.get('typeName', [])\n",
    "            if types is None:\n",
    "                #print(f\"No types for: {document.keys()}, {document['label']}\")\n",
    "                types = \"\"\n",
    "                \n",
    "            # The label / name for this candidate\n",
    "            label = document.get('label', [])\n",
    "            \n",
    "            # URI for this candidate\n",
    "            uri = document.get('resource', [])\n",
    "\n",
    "            # Score (Optional)\n",
    "            score = document.get('score', [])\n",
    "\n",
    "            # Reference Count (Optional)\n",
    "            refCount = document.get('refCount', [])\n",
    "\n",
    "            # Populate information\n",
    "            candidate_dict['types'].append(types)\n",
    "            candidate_dict['label'].extend(label)\n",
    "            candidate_dict['desc'].extend(small_desc)\n",
    "            candidate_dict['uri'].extend(uri)\n",
    "            candidate_dict['score'].extend(score)\n",
    "            candidate_dict['refCount'].extend(refCount)\n",
    "\n",
    "            \n",
    "        except:\n",
    "            print(f\"ERROR: {document.keys()}\")\n",
    "            raise\n",
    "    return candidate_dict['label'], candidate_dict['desc'], candidate_dict['types'], candidate_dict['uri'], candidate_dict['score'], candidate_dict['refCount']\n",
    "\n",
    "\n",
    "#labels, desc, types, uris, score, refCount = transform_dbp_json_cands_to_dict(json_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_candidates(mention):\n",
    "    endpoint_url = \"https://lookup.dbpedia.org/api/search?format=JSON&query=\"\n",
    "\n",
    "    response = requests.get(endpoint_url+str(mention))\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Could not retrieve candidates for mention[{mention}], code: {response.status_code}\")\n",
    "    # Response to JSON\n",
    "    json_candidates = json.loads(response.text)\n",
    "    \n",
    "    # Extract the candidate \"documents\" from the response JSON\n",
    "    json_candidate_documents = json_candidates['docs']\n",
    "    \n",
    "    # Transform DBpedia JSON candidates to data structure we want to use for LLM purposes\n",
    "    labels, desc, types, uris, score, refCount = transform_dbp_json_cands_to_dict(json_documents=json_candidate_documents)\n",
    "    return [labels, desc, types, uris, score, refCount]\n",
    "\n",
    "#lst_candidate_info = generate_candidates(\"Steve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see some type names\n",
    "#set([x for y in candidate_dict['types'] for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def save_candidate_result(cand_dictionary={}, \\\n",
    "                          result_directory=\"C:/Users/wf7467/Desktop/GitHub/KIT/agnostic-disambiguation/data/candidates/CoNLL_AIDA-YAGO2-dataset.nif/\",\\\n",
    "                          file_counter=\"NA\"):\n",
    "    with open(result_directory+str(file_counter)+'.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(cand_dictionary, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "def load_candidate_results(\n",
    "                          result_directory=\"C:/Users/wf7467/Desktop/GitHub/KIT/agnostic-disambiguation/data/candidates/CoNLL_AIDA-YAGO2-dataset.nif/\"):\n",
    "    all_candidate_results = {}\n",
    "    for path in [result_directory+f for f in listdir(result_directory) if isfile(join(result_directory, f))]:\n",
    "        with open(path, encoding='utf-8') as json_file:\n",
    "            in_json = json.load(json_file)\n",
    "            \n",
    "            all_candidate_results[path] = in_json\n",
    "    return all_candidate_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mention data\n",
    "from pynif import NIFCollection\n",
    "\n",
    "def load_nif_dataset(nif_data_path = \"C:\\\\Users\\\\wf7467\\\\Desktop\\\\Evaluation Datasets\\\\Datasets\\\\entity_linking\\\\conll_aida-yago2-dataset\\\\AIDA-YAGO2-dataset.tsv_nif\"):\n",
    "\n",
    "    nif_data = \"\"\n",
    "\n",
    "    with open(nif_data_path, 'r', encoding=\"utf-8\") as f:\n",
    "        nif_data = f.read()\n",
    "\n",
    "    parsed_collection = NIFCollection.loads(nif_data, format='turtle')\n",
    "\n",
    "parsed_collection = load_nif_dataset(nif_data_path = \"C:\\\\Users\\\\wf7467\\\\Desktop\\\\Evaluation Datasets\\\\Datasets\\\\entity_linking\\\\conll_aida-yago2-dataset\\\\AIDA-YAGO2-dataset.tsv_nif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dictionary of results that will be persisted for generating prompts from\n",
    "\n",
    "#from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def compute_save_candidates_for_mentions(result_directory=\"C:/Users/wf7467/Desktop/GitHub/KIT/agnostic-disambiguation/data/candidates/CoNLL_AIDA-YAGO2-dataset.nif/\"):\n",
    "# Check which are done in directory so we don't redo them\n",
    "\n",
    "    # to find what is where & avoid redoing\n",
    "    mapping_path_document = {}\n",
    "    for path in [result_directory+f for f in listdir(result_directory) if isfile(join(result_directory, f))]:\n",
    "        with open(path, encoding='utf-8') as json_file:\n",
    "            in_json = json.load(json_file)\n",
    "        key = list(dict(in_json).keys())[0]\n",
    "        mapping_path_document[key] = path\n",
    "\n",
    "    file_counter = len(mapping_path_document)\n",
    "    # To see how many were skipped\n",
    "    skip_counter = 0\n",
    "    \n",
    "    for context in tqdm_notebook(parsed_collection.contexts, position=0):\n",
    "        mapping_document_candidate_info = {}\n",
    "\n",
    "        # See what can be called on context\n",
    "        # print(dir(context))\n",
    "\n",
    "        # document/context.mention = input sentence (1 document = 1 sentence)\n",
    "        document = context.mention\n",
    "        # If it's done already, continue to next\n",
    "        if document in mapping_path_document.keys():\n",
    "            skip_counter += 1\n",
    "            continue\n",
    "\n",
    "        # Document string will be used as key for all candidate information\n",
    "        key = document\n",
    "\n",
    "        mentions = []\n",
    "\n",
    "        doc_labels = []\n",
    "        doc_desc = []\n",
    "        doc_types = []\n",
    "        doc_candidate_uris = []\n",
    "        doc_scores = []\n",
    "        doc_refCount = []\n",
    "        doc_true_uris = []\n",
    "        doc_begin_index = []\n",
    "        doc_end_index = []\n",
    "\n",
    "\n",
    "        #print(f\"{context.mention}, {context.beginIndex}, {context.endIndex}\")\n",
    "        # 1 phrase = 1 mention\n",
    "        for phrase in tqdm_notebook(context.phrases, position=1):\n",
    "            #print(phrase)\n",
    "            #print(dir(phrase))\n",
    "            #print(f\"{phrase.mention}, {phrase.beginIndex}, {phrase.endIndex}, {phrase.generated_uri}, {phrase.taClassRef}, {phrase.taIdentRef}\")\n",
    "            lst_candidate_info = generate_candidates(phrase.mention)\n",
    "            labels = lst_candidate_info[0]\n",
    "            desc = lst_candidate_info[1]\n",
    "            types = lst_candidate_info[2]\n",
    "            candidate_uris = lst_candidate_info[3]\n",
    "            scores = lst_candidate_info[4]\n",
    "            refCount = lst_candidate_info[5]\n",
    "            # Single URI\n",
    "            true_uri = phrase.taIdentRef\n",
    "\n",
    "            # Add data for persistence\n",
    "            # always single entry per mention --> extend\n",
    "            # multiple entries per mention    --> append\n",
    "            mentions.append(phrase.mention)\n",
    "            doc_labels.append(labels)\n",
    "            doc_desc.append(desc)\n",
    "            doc_types.append(types)\n",
    "            doc_candidate_uris.append(candidate_uris)\n",
    "            doc_scores.append(scores)\n",
    "            doc_refCount.append(refCount)\n",
    "            # Single URI\n",
    "            doc_true_uris.append(true_uri)\n",
    "            doc_begin_index.append(phrase.beginIndex)\n",
    "            doc_end_index.append(phrase.endIndex)\n",
    "\n",
    "\n",
    "        # Once all phrases aka. mentions for a sentence are done, store the sentence information\n",
    "        mapping_document_candidate_info[key] = {\n",
    "            'mentions' : mentions,#\n",
    "            'begin_index' : doc_begin_index,#\n",
    "            'end_index' : doc_end_index,#\n",
    "            'labels' : doc_labels,#\n",
    "            'desc' : doc_desc,#\n",
    "            'types' : doc_types,#\n",
    "            'candidate_uris' : doc_candidate_uris,#\n",
    "            'scores' : doc_scores,#\n",
    "            'refCount' : doc_refCount,#\n",
    "            'true_uris' : doc_true_uris,#\n",
    "        }\n",
    "\n",
    "        # Save results since sentence has been processed\n",
    "\n",
    "        save_candidate_result(cand_dictionary=mapping_document_candidate_info, file_counter=file_counter)\n",
    "        file_counter+=1\n",
    "        #if file_counter >= 2:\n",
    "        #    break\n",
    "\n",
    "    print(f\"Finished processing {file_counter} documents, skipped {skip_counter}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1393\n"
     ]
    }
   ],
   "source": [
    "all_candidate_results = load_candidate_results(result_directory=\"C:/Users/wf7467/Desktop/GitHub/KIT/agnostic-disambiguation/data/candidates/CoNLL_AIDA-YAGO2-dataset.nif/\")\n",
    "print(f\"Number of entries: {len(all_candidate_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/wf7467/Desktop/GitHub/KIT/agnostic-disambiguation/data/candidates/CoNLL_AIDA-YAGO2-dataset.nif/1006.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['mentions', 'begin_index', 'end_index', 'labels', 'desc', 'types', 'candidate_uris', 'scores', 'refCount', 'true_uris'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get some random key to get one entry\n",
    "key = (list(all_candidate_results.keys()))[10]\n",
    "#print(all_candidate_results[list(all_candidate_results.keys())[10]])\n",
    "print(key)\n",
    "#key = next(iter(all_candidate_results))\n",
    "candidates_entry = all_candidate_results[key]\n",
    "# Only has one key\n",
    "in_text = next(iter( candidates_entry ))\n",
    "json_candidates = candidates_entry[in_text]\n",
    "json_candidates.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through all entries, grab the URIs ('candidate_uris' and 'true_uris')\n",
    "# Make a set out of them\n",
    "# Query SPARQL for their descriptions via DBpedia\n",
    "# Save them all in a dictionary fashion: URI --> text description\n",
    "# Save to file\n",
    "# Load it properly\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3944986\n",
      "361239\n",
      "http://dbpedia.org/resource/Paul_Whelan\n"
     ]
    }
   ],
   "source": [
    "# Step 1: For which URIs do we need additional info (aka. descriptions)?\n",
    "\n",
    "wanted_keys = ['candidate_uris', 'true_uris']\n",
    "lst_len_counter = 0\n",
    "uris_for_descriptions = set()\n",
    "\n",
    "for in_file in all_candidate_results:\n",
    "    entry = all_candidate_results[in_file]\n",
    "    #print(in_file, entry.keys())\n",
    "\n",
    "    for document in entry:\n",
    "        document_candidate_json = entry[document]\n",
    "        for key in document_candidate_json:\n",
    "            if key in wanted_keys:\n",
    "                uris_of_uris = document_candidate_json[key]\n",
    "                for uris in uris_of_uris:\n",
    "                    lst_len_counter = lst_len_counter + len(uris)\n",
    "                    uris_for_descriptions.update(uris)\n",
    "                #print(len(uris_for_descriptions))\n",
    "                #print(lst_len_counter)\n",
    "            #break\n",
    "        #break\n",
    "    #break\n",
    "\n",
    "print(lst_len_counter)\n",
    "print(len(uris_for_descriptions))\n",
    "print(next(iter(uris_for_descriptions)))\n",
    "uris_for_descriptions = list(uris_for_descriptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Query SPARQL for their descriptions via DBpedia\n",
    "# https://sparqlwrapper.readthedocs.io/en/latest/main.html\n",
    "#!pip install sparqlwrapper\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, POST\n",
    "\n",
    "def build_get_description(dbpedia_subject_uri=None):\n",
    "    if dbpedia_subject_uri is None:\n",
    "        raise ValueError(\"You need to input a subject URI/IRI\")\n",
    "    return \"\"\"\n",
    "        {\n",
    "            ?s dbo:abstract ?desc .\n",
    "            FILTER(?s = <\"\"\"+dbpedia_subject_uri+\"\"\">) . \n",
    "            FILTER(LANG(?desc) = \"en\") .\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "def build_union_query_get_description(dbpedia_subject_uris=[]):\n",
    "    if dbpedia_subject_uris is None:\n",
    "        raise ValueError(\"You need to input a subject URI/IRI\")\n",
    "    union_query_start = \"\"\"\n",
    "    SELECT DISTINCT ?s ?desc\n",
    "    WHERE\n",
    "    {\n",
    "    \"\"\"\n",
    "    center_query = \"\"\n",
    "    union_query_end = \"}\"\n",
    "\n",
    "    # Take the first element separately... makes it a lot easier for the logic\n",
    "    center_query += build_get_description(str(dbpedia_subject_uris[0]))\n",
    "\n",
    "    for idx in range(1, len(dbpedia_subject_uris)):\n",
    "        uri = str(dbpedia_subject_uris[idx])\n",
    "        center_query += \"\"\"\n",
    "        UNION\n",
    "        \"\"\"\n",
    "        center_query += build_get_description(uri)\n",
    "    return union_query_start + center_query + union_query_end\n",
    "\n",
    "\n",
    "def query_descriptions_multiple_uris(uris_for_descriptions):\n",
    "    '''Note that DBpedia's SPARQL endpoint can only handle 10k output - so definitely make it smaller than that.'''\n",
    "    query = build_union_query_get_description(uris_for_descriptions)\n",
    "\n",
    "    sparql = SPARQLWrapper(endpoint=\"https://dbpedia.org/sparql\", returnFormat=JSON)\n",
    "    sparql.setMethod(POST)\n",
    "\n",
    "    # test_query = \"\"\"\n",
    "    #     SELECT DISTINCT ?s ?desc\n",
    "    #     WHERE\n",
    "    #     {\n",
    "    #         { \n",
    "    #             ?s dbo:abstract ?desc .\n",
    "    #             FILTER(?s = <http://dbpedia.org/resource/Steve_Jobs>) . \n",
    "    #             FILTER(LANG(?desc) = \"en\") .\n",
    "    #         }\n",
    "    #         UNION\n",
    "    #         { \n",
    "    #             ?s dbo:abstract ?desc .\n",
    "    #             FILTER(?s = <http://dbpedia.org/resource/Steve_Wozniak>) . \n",
    "    #             FILTER(LANG(?desc) = \"en\") .\n",
    "    #         }\n",
    "    #     }\n",
    "    #     \"\"\"\n",
    "\n",
    "    sparql.setQuery(query)\n",
    "\n",
    "    dict_uri_desc = {}\n",
    "\n",
    "    try:\n",
    "        ret = sparql.queryAndConvert()\n",
    "\n",
    "\n",
    "        for r in ret[\"results\"][\"bindings\"]:\n",
    "            uri = r['s']['value']\n",
    "            desc = r['desc']['value']\n",
    "            dict_uri_desc[uri] = desc\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return dict_uri_desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def save_cand_desc_result(cand_desc_dictionary={}, \\\n",
    "                          result_directory=\"C:/Users/wf7467/Desktop/GitHub/KIT/agnostic-disambiguation/data/candidate_descriptions/CoNLL_AIDA-YAGO2-dataset.nif/\",\\\n",
    "                          file_counter=\"NA\"):\n",
    "    with open(result_directory+str(file_counter)+'.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(cand_desc_dictionary, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "def load_cand_desc_results(\n",
    "                          result_directory=\"C:/Users/wf7467/Desktop/GitHub/KIT/agnostic-disambiguation/data/candidate_descriptions/CoNLL_AIDA-YAGO2-dataset.nif/\"):\n",
    "    all_cand_desc_results = {}\n",
    "    for path in [result_directory+f for f in listdir(result_directory) if isfile(join(result_directory, f))]:\n",
    "        with open(path, encoding='utf-8') as json_file:\n",
    "            in_json = json.load(json_file)\n",
    "            \n",
    "            all_cand_desc_results[path] = in_json\n",
    "    return all_cand_desc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on range[0, 100]\n",
      "Working on range[100, 200]\n",
      "Working on range[200, 300]\n",
      "Working on range[300, 400]\n",
      "Working on range[400, 500]\n",
      "Working on range[500, 600]\n",
      "Working on range[600, 700]\n",
      "Working on range[700, 800]\n",
      "Working on range[800, 900]\n",
      "Working on range[900, 1000]\n",
      "Working on range[1000, 1100]\n",
      "Working on range[1100, 1200]\n",
      "Working on range[1200, 1300]\n",
      "Working on range[1300, 1400]\n",
      "Working on range[1400, 1500]\n",
      "Working on range[1500, 1600]\n",
      "Working on range[1600, 1700]\n",
      "Working on range[1700, 1800]\n",
      "Working on range[1800, 1900]\n",
      "Working on range[1900, 2000]\n",
      "Working on range[2000, 2100]\n",
      "Working on range[2100, 2200]\n",
      "Working on range[2200, 2300]\n",
      "Working on range[2300, 2400]\n",
      "Working on range[2400, 2500]\n",
      "Working on range[2500, 2600]\n",
      "Working on range[2600, 2700]\n",
      "Working on range[2700, 2800]\n",
      "Working on range[2800, 2900]\n",
      "Working on range[2900, 3000]\n",
      "Working on range[3000, 3100]\n",
      "Working on range[3100, 3200]\n",
      "Working on range[3200, 3300]\n",
      "Working on range[3300, 3400]\n",
      "Working on range[3400, 3500]\n",
      "Working on range[3500, 3600]\n",
      "Working on range[3600, 3700]\n",
      "Working on range[3700, 3800]\n",
      "Working on range[3800, 3900]\n",
      "Working on range[3900, 4000]\n",
      "Working on range[4000, 4100]\n",
      "Working on range[4100, 4200]\n",
      "Working on range[4200, 4300]\n",
      "Working on range[4300, 4400]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(uris_for_descriptions), step):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorking on range[\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (i, i\u001b[38;5;241m+\u001b[39mstep))\n\u001b[1;32m----> 6\u001b[0m     dict_uri_desc \u001b[38;5;241m=\u001b[39m \u001b[43mquery_descriptions_multiple_uris\u001b[49m\u001b[43m(\u001b[49m\u001b[43muris_for_descriptions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     save_cand_desc_result(cand_desc_dictionary\u001b[38;5;241m=\u001b[39mdict_uri_desc, file_counter\u001b[38;5;241m=\u001b[39mi)\n\u001b[0;32m      8\u001b[0m     file_counter_desc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[99], line 70\u001b[0m, in \u001b[0;36mquery_descriptions_multiple_uris\u001b[1;34m(uris_for_descriptions)\u001b[0m\n\u001b[0;32m     67\u001b[0m dict_uri_desc \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43msparql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryAndConvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbindings\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     74\u001b[0m         uri \u001b[38;5;241m=\u001b[39m r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\wf7467\\Desktop\\Code\\agnostic-disambiguation\\.venv\\lib\\site-packages\\SPARQLWrapper\\Wrapper.py:968\u001b[0m, in \u001b[0;36mSPARQLWrapper.queryAndConvert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Macro like method: issue a query and return the converted results.\u001b[39;00m\n\u001b[0;32m    964\u001b[0m \n\u001b[0;32m    965\u001b[0m \u001b[38;5;124;03m:return: the converted query result. See the conversion methods for more details.\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    967\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery()\n\u001b[1;32m--> 968\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wf7467\\Desktop\\Code\\agnostic-disambiguation\\.venv\\lib\\site-packages\\SPARQLWrapper\\Wrapper.py:1196\u001b[0m, in \u001b[0;36mQueryResult.convert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _content_type_in_list(ct, _SPARQL_JSON):\n\u001b[0;32m   1195\u001b[0m     _validate_format(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJSON\u001b[39m\u001b[38;5;124m\"\u001b[39m, [JSON], ct, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequestedFormat)\n\u001b[1;32m-> 1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convertJSON\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _content_type_in_list(ct, _RDF_XML):\n\u001b[0;32m   1198\u001b[0m     _validate_format(\n\u001b[0;32m   1199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDF/XML\u001b[39m\u001b[38;5;124m\"\u001b[39m, [RDF, XML, RDFXML], ct, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequestedFormat\n\u001b[0;32m   1200\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\wf7467\\Desktop\\Code\\agnostic-disambiguation\\.venv\\lib\\site-packages\\SPARQLWrapper\\Wrapper.py:1059\u001b[0m, in \u001b[0;36mQueryResult._convertJSON\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convertJSON\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[Any, Any]:\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;124;03m    Convert a JSON result into a Python dict. This method can be overwritten in a subclass\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m    for a different conversion method.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;124;03m    :rtype: dict\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1059\u001b[0m     json_str \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1060\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(json_str, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1061\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json_str\n",
      "File \u001b[1;32m~\\Desktop\\dev_programs\\python3\\64bit\\python3install\\lib\\http\\client.py:467\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead:\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\Desktop\\dev_programs\\python3\\64bit\\python3install\\lib\\http\\client.py:608\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[0;32m    602\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[0;32m    610\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[1;32m~\\Desktop\\dev_programs\\python3\\64bit\\python3install\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\dev_programs\\python3\\64bit\\python3install\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\Desktop\\dev_programs\\python3\\64bit\\python3install\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step of 100 would hopefully work\n",
    "step = 100\n",
    "file_counter_desc = 0\n",
    "for i in range(0, len(uris_for_descriptions), step):\n",
    "    print(\"Working on range[%d, %d]\" % (i, i+step))\n",
    "    dict_uri_desc = query_descriptions_multiple_uris(uris_for_descriptions[i:i+step])\n",
    "    save_cand_desc_result(cand_desc_dictionary=dict_uri_desc, file_counter=i)\n",
    "    file_counter_desc += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now double check whether all of the URIs we have also actually have an associated description\n",
    "# If not, ... crawl the missing ones again.\n",
    "def check_for_missing_uri_descriptions_and_complete(uris_for_descriptions=[], step=100, prefix=\"\"):\n",
    "    all_cand_desc_results = load_cand_desc_results()\n",
    "\n",
    "    # uris_for_descriptions --> all URIs we want to crawl\n",
    "    # cands_desc\n",
    "    uris_for_descriptions_crawled = set()\n",
    "    for filename in all_cand_desc_results:\n",
    "        cands_desc = all_cand_desc_results[filename]\n",
    "        #print(list(cands_desc.keys())[0])\n",
    "        uris_for_descriptions_crawled.update(set(cands_desc.keys()))\n",
    "\n",
    "    set_missing_uris = set(uris_for_descriptions) - set(uris_for_descriptions_crawled)\n",
    "    lst_missing_uris = list(set_missing_uris)\n",
    "    #print(lst_missing_uris[0:50])\n",
    "\n",
    "    print(\"%d missing URIs (crawled: %d, total wanted: %d).\" % (len(set_missing_uris), len(set(uris_for_descriptions_crawled)), len(set(uris_for_descriptions))))\n",
    "\n",
    "    for i in range(0, len(lst_missing_uris), step):\n",
    "        print(\"Working on missing range[%d, %d]\" % (i, i+step))\n",
    "        dict_uri_desc = query_descriptions_multiple_uris(lst_missing_uris[i:i+step])\n",
    "        save_cand_desc_result(cand_desc_dictionary=dict_uri_desc, file_counter=prefix+str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://dbpedia.org/resource/Paul_Whelan', 'http://dbpedia.org/resource/Rovereto_(disambiguation)', 'http://dbpedia.org/resource/Irwin', 'http://dbpedia.org/resource/CEFCU', 'http://dbpedia.org/resource/Angolan_swallow', 'http://dbpedia.org/resource/Mountain_View,_New_Mexico', 'http://dbpedia.org/resource/Siege_of_Damascus', 'http://dbpedia.org/resource/Jerry_Simmons', 'http://dbpedia.org/resource/Frank_Richter', 'http://dbpedia.org/resource/CGU', 'http://dbpedia.org/resource/El_Gran_campeón', 'http://dbpedia.org/resource/Tsentaroy', 'http://dbpedia.org/resource/DoÃ±a_Ana_County,_New_Mexico', 'http://dbpedia.org/resource/Bethlehem_High_School', 'http://dbpedia.org/resource/List_of_unnumbered_minor_planets:_2003_A–R', 'http://dbpedia.org/resource/SPAP', 'http://dbpedia.org/resource/Charleston_Civic_Center', 'http://dbpedia.org/resource/ASAC', 'http://dbpedia.org/resource/Robert_Irwin', 'http://dbpedia.org/resource/Maarit_LepomÃ¤ki', 'http://dbpedia.org/resource/PloieÈ™ti', 'http://dbpedia.org/resource/Quattro_Canti,_Catania', 'http://dbpedia.org/resource/Peruvian_Brazilians', \"http://dbpedia.org/resource/Church_of_Sant'Angelo,_Perugia\", 'http://dbpedia.org/resource/Yelena_Vyalbe', 'http://dbpedia.org/resource/Saint-cyriens', 'http://dbpedia.org/resource/1994_Milan_Indoor_–_Singles', 'http://dbpedia.org/resource/Tuqtu', 'http://dbpedia.org/resource/Military_of_Mauritius', 'http://dbpedia.org/resource/Alistar_Fredericks', \"http://dbpedia.org/resource/2002_Kroger_St._Jude_International_–_Women's_Doubles\", 'http://dbpedia.org/resource/Wayne_Simmons', 'http://dbpedia.org/resource/Lindsay_(name)', 'http://dbpedia.org/resource/List_of_freshman_class_members_of_the_111th_United_States_Congress', 'http://dbpedia.org/resource/Neohumanism', 'http://dbpedia.org/resource/Kenan_IÅŸÄ±k', 'http://dbpedia.org/resource/Belfast_Duncairn', 'http://dbpedia.org/resource/Hum_Sab_Chor_Hain', 'http://dbpedia.org/resource/Flamengo_(disambiguation)', 'http://dbpedia.org/resource/Hà_Tĩnh_Province', 'http://dbpedia.org/resource/List_of_KDE_applications', 'http://dbpedia.org/resource/Sandusky_(surname)', 'http://dbpedia.org/resource/Kishibe', 'http://dbpedia.org/resource/Toyota_concept_vehicles_(1970–1979)', 'http://dbpedia.org/resource/FranÃ§ois_PÃ©ron', 'http://dbpedia.org/resource/WGCL-TV', \"http://dbpedia.org/resource/2017_Ricoh_Open_–_Women's_Doubles\", 'http://dbpedia.org/resource/KB_Financial_Group_Inc', 'http://dbpedia.org/resource/Arjun_Singh_(politician,_born_1962)', 'http://dbpedia.org/resource/103rd_Infantry_Division_Piacenza']\n",
      "30614 missing URIs (crawled: 330625, total wanted: 361239).\n",
      "Working on missing range[0, 100]\n",
      "Working on missing range[100, 200]\n",
      "Working on missing range[200, 300]\n",
      "Working on missing range[300, 400]\n",
      "Working on missing range[400, 500]\n",
      "Working on missing range[500, 600]\n",
      "Working on missing range[600, 700]\n",
      "Working on missing range[700, 800]\n",
      "Working on missing range[800, 900]\n",
      "Working on missing range[900, 1000]\n",
      "Working on missing range[1000, 1100]\n",
      "Working on missing range[1100, 1200]\n",
      "Working on missing range[1200, 1300]\n",
      "Working on missing range[1300, 1400]\n",
      "Working on missing range[1400, 1500]\n",
      "Working on missing range[1500, 1600]\n",
      "Working on missing range[1600, 1700]\n",
      "Working on missing range[1700, 1800]\n",
      "Working on missing range[1800, 1900]\n",
      "Working on missing range[1900, 2000]\n",
      "Working on missing range[2000, 2100]\n",
      "Working on missing range[2100, 2200]\n",
      "Working on missing range[2200, 2300]\n",
      "Working on missing range[2300, 2400]\n",
      "Working on missing range[2400, 2500]\n",
      "Working on missing range[2500, 2600]\n",
      "Working on missing range[2600, 2700]\n",
      "Working on missing range[2700, 2800]\n",
      "Working on missing range[2800, 2900]\n",
      "Working on missing range[2900, 3000]\n",
      "Working on missing range[3000, 3100]\n",
      "Working on missing range[3100, 3200]\n",
      "Working on missing range[3200, 3300]\n",
      "Working on missing range[3300, 3400]\n",
      "Working on missing range[3400, 3500]\n",
      "Working on missing range[3500, 3600]\n",
      "Working on missing range[3600, 3700]\n",
      "Working on missing range[3700, 3800]\n",
      "Working on missing range[3800, 3900]\n",
      "Working on missing range[3900, 4000]\n",
      "Working on missing range[4000, 4100]\n",
      "Working on missing range[4100, 4200]\n",
      "Working on missing range[4200, 4300]\n",
      "Working on missing range[4300, 4400]\n",
      "Working on missing range[4400, 4500]\n",
      "Working on missing range[4500, 4600]\n",
      "Working on missing range[4600, 4700]\n",
      "Working on missing range[4700, 4800]\n",
      "Working on missing range[4800, 4900]\n",
      "Working on missing range[4900, 5000]\n",
      "Working on missing range[5000, 5100]\n",
      "Working on missing range[5100, 5200]\n",
      "Working on missing range[5200, 5300]\n",
      "Working on missing range[5300, 5400]\n",
      "Working on missing range[5400, 5500]\n",
      "Working on missing range[5500, 5600]\n",
      "Working on missing range[5600, 5700]\n",
      "Working on missing range[5700, 5800]\n",
      "Working on missing range[5800, 5900]\n",
      "Working on missing range[5900, 6000]\n",
      "Working on missing range[6000, 6100]\n",
      "Working on missing range[6100, 6200]\n",
      "Working on missing range[6200, 6300]\n",
      "Working on missing range[6300, 6400]\n",
      "Working on missing range[6400, 6500]\n",
      "Working on missing range[6500, 6600]\n",
      "Working on missing range[6600, 6700]\n",
      "Working on missing range[6700, 6800]\n",
      "Working on missing range[6800, 6900]\n",
      "Working on missing range[6900, 7000]\n",
      "Working on missing range[7000, 7100]\n",
      "Working on missing range[7100, 7200]\n",
      "Working on missing range[7200, 7300]\n",
      "Working on missing range[7300, 7400]\n",
      "Working on missing range[7400, 7500]\n",
      "Working on missing range[7500, 7600]\n",
      "Working on missing range[7600, 7700]\n",
      "Working on missing range[7700, 7800]\n",
      "Working on missing range[7800, 7900]\n",
      "Working on missing range[7900, 8000]\n",
      "Working on missing range[8000, 8100]\n",
      "Working on missing range[8100, 8200]\n",
      "Working on missing range[8200, 8300]\n",
      "Working on missing range[8300, 8400]\n",
      "Working on missing range[8400, 8500]\n",
      "Working on missing range[8500, 8600]\n",
      "Working on missing range[8600, 8700]\n",
      "Working on missing range[8700, 8800]\n",
      "Working on missing range[8800, 8900]\n",
      "Working on missing range[8900, 9000]\n",
      "Working on missing range[9000, 9100]\n",
      "Working on missing range[9100, 9200]\n",
      "Working on missing range[9200, 9300]\n",
      "Working on missing range[9300, 9400]\n",
      "Working on missing range[9400, 9500]\n",
      "Working on missing range[9500, 9600]\n",
      "Working on missing range[9600, 9700]\n",
      "Working on missing range[9700, 9800]\n",
      "Working on missing range[9800, 9900]\n",
      "Working on missing range[9900, 10000]\n",
      "Working on missing range[10000, 10100]\n",
      "Working on missing range[10100, 10200]\n",
      "Working on missing range[10200, 10300]\n",
      "Working on missing range[10300, 10400]\n",
      "Working on missing range[10400, 10500]\n",
      "Working on missing range[10500, 10600]\n",
      "Working on missing range[10600, 10700]\n",
      "Working on missing range[10700, 10800]\n",
      "Working on missing range[10800, 10900]\n",
      "Working on missing range[10900, 11000]\n",
      "Working on missing range[11000, 11100]\n",
      "Working on missing range[11100, 11200]\n",
      "Working on missing range[11200, 11300]\n",
      "Working on missing range[11300, 11400]\n",
      "Working on missing range[11400, 11500]\n",
      "Working on missing range[11500, 11600]\n",
      "Working on missing range[11600, 11700]\n",
      "Working on missing range[11700, 11800]\n",
      "Working on missing range[11800, 11900]\n",
      "Working on missing range[11900, 12000]\n",
      "Working on missing range[12000, 12100]\n",
      "Working on missing range[12100, 12200]\n",
      "Working on missing range[12200, 12300]\n",
      "Working on missing range[12300, 12400]\n",
      "Working on missing range[12400, 12500]\n",
      "Working on missing range[12500, 12600]\n",
      "Working on missing range[12600, 12700]\n",
      "Working on missing range[12700, 12800]\n",
      "Working on missing range[12800, 12900]\n",
      "Working on missing range[12900, 13000]\n",
      "Working on missing range[13000, 13100]\n",
      "Working on missing range[13100, 13200]\n",
      "Working on missing range[13200, 13300]\n",
      "Working on missing range[13300, 13400]\n",
      "Working on missing range[13400, 13500]\n",
      "Working on missing range[13500, 13600]\n",
      "Working on missing range[13600, 13700]\n",
      "Working on missing range[13700, 13800]\n",
      "Working on missing range[13800, 13900]\n",
      "Working on missing range[13900, 14000]\n",
      "Working on missing range[14000, 14100]\n",
      "Working on missing range[14100, 14200]\n",
      "Working on missing range[14200, 14300]\n",
      "Working on missing range[14300, 14400]\n",
      "Working on missing range[14400, 14500]\n",
      "Working on missing range[14500, 14600]\n",
      "Working on missing range[14600, 14700]\n",
      "Working on missing range[14700, 14800]\n",
      "Working on missing range[14800, 14900]\n",
      "Working on missing range[14900, 15000]\n",
      "Working on missing range[15000, 15100]\n",
      "Working on missing range[15100, 15200]\n",
      "Working on missing range[15200, 15300]\n",
      "Working on missing range[15300, 15400]\n",
      "Working on missing range[15400, 15500]\n",
      "Working on missing range[15500, 15600]\n",
      "Working on missing range[15600, 15700]\n",
      "Working on missing range[15700, 15800]\n",
      "Working on missing range[15800, 15900]\n",
      "Working on missing range[15900, 16000]\n",
      "Working on missing range[16000, 16100]\n",
      "Working on missing range[16100, 16200]\n",
      "Working on missing range[16200, 16300]\n",
      "Working on missing range[16300, 16400]\n",
      "Working on missing range[16400, 16500]\n",
      "Working on missing range[16500, 16600]\n",
      "Working on missing range[16600, 16700]\n",
      "Working on missing range[16700, 16800]\n",
      "Working on missing range[16800, 16900]\n",
      "Working on missing range[16900, 17000]\n",
      "Working on missing range[17000, 17100]\n",
      "Working on missing range[17100, 17200]\n",
      "Working on missing range[17200, 17300]\n",
      "Working on missing range[17300, 17400]\n",
      "Working on missing range[17400, 17500]\n",
      "Working on missing range[17500, 17600]\n",
      "Working on missing range[17600, 17700]\n",
      "Working on missing range[17700, 17800]\n",
      "Working on missing range[17800, 17900]\n",
      "Working on missing range[17900, 18000]\n",
      "Working on missing range[18000, 18100]\n",
      "Working on missing range[18100, 18200]\n",
      "Working on missing range[18200, 18300]\n",
      "Working on missing range[18300, 18400]\n",
      "Working on missing range[18400, 18500]\n",
      "Working on missing range[18500, 18600]\n",
      "Working on missing range[18600, 18700]\n",
      "Working on missing range[18700, 18800]\n",
      "Working on missing range[18800, 18900]\n",
      "Working on missing range[18900, 19000]\n",
      "Working on missing range[19000, 19100]\n",
      "Working on missing range[19100, 19200]\n",
      "Working on missing range[19200, 19300]\n",
      "Working on missing range[19300, 19400]\n",
      "Working on missing range[19400, 19500]\n",
      "Working on missing range[19500, 19600]\n",
      "Working on missing range[19600, 19700]\n",
      "Working on missing range[19700, 19800]\n",
      "Working on missing range[19800, 19900]\n",
      "Working on missing range[19900, 20000]\n",
      "Working on missing range[20000, 20100]\n",
      "Working on missing range[20100, 20200]\n",
      "Working on missing range[20200, 20300]\n",
      "Working on missing range[20300, 20400]\n",
      "Working on missing range[20400, 20500]\n",
      "Working on missing range[20500, 20600]\n",
      "Working on missing range[20600, 20700]\n",
      "Working on missing range[20700, 20800]\n",
      "Working on missing range[20800, 20900]\n",
      "Working on missing range[20900, 21000]\n",
      "Working on missing range[21000, 21100]\n",
      "Working on missing range[21100, 21200]\n",
      "Working on missing range[21200, 21300]\n",
      "Working on missing range[21300, 21400]\n",
      "Working on missing range[21400, 21500]\n",
      "Working on missing range[21500, 21600]\n",
      "Working on missing range[21600, 21700]\n",
      "Working on missing range[21700, 21800]\n",
      "Working on missing range[21800, 21900]\n",
      "Working on missing range[21900, 22000]\n",
      "Working on missing range[22000, 22100]\n",
      "Working on missing range[22100, 22200]\n",
      "Working on missing range[22200, 22300]\n",
      "Working on missing range[22300, 22400]\n",
      "Working on missing range[22400, 22500]\n",
      "Working on missing range[22500, 22600]\n",
      "Working on missing range[22600, 22700]\n",
      "Working on missing range[22700, 22800]\n",
      "Working on missing range[22800, 22900]\n",
      "Working on missing range[22900, 23000]\n",
      "Working on missing range[23000, 23100]\n",
      "Working on missing range[23100, 23200]\n",
      "Working on missing range[23200, 23300]\n",
      "Working on missing range[23300, 23400]\n",
      "Working on missing range[23400, 23500]\n",
      "Working on missing range[23500, 23600]\n",
      "Working on missing range[23600, 23700]\n",
      "Working on missing range[23700, 23800]\n",
      "Working on missing range[23800, 23900]\n",
      "Working on missing range[23900, 24000]\n",
      "Working on missing range[24000, 24100]\n",
      "Working on missing range[24100, 24200]\n",
      "Working on missing range[24200, 24300]\n",
      "Working on missing range[24300, 24400]\n",
      "Working on missing range[24400, 24500]\n",
      "Working on missing range[24500, 24600]\n",
      "Working on missing range[24600, 24700]\n",
      "Working on missing range[24700, 24800]\n",
      "Working on missing range[24800, 24900]\n",
      "Working on missing range[24900, 25000]\n",
      "Working on missing range[25000, 25100]\n",
      "Working on missing range[25100, 25200]\n",
      "Working on missing range[25200, 25300]\n",
      "Working on missing range[25300, 25400]\n",
      "Working on missing range[25400, 25500]\n",
      "Working on missing range[25500, 25600]\n",
      "Working on missing range[25600, 25700]\n",
      "Working on missing range[25700, 25800]\n",
      "Working on missing range[25800, 25900]\n",
      "Working on missing range[25900, 26000]\n",
      "Working on missing range[26000, 26100]\n",
      "Working on missing range[26100, 26200]\n",
      "Working on missing range[26200, 26300]\n",
      "Working on missing range[26300, 26400]\n",
      "Working on missing range[26400, 26500]\n",
      "Working on missing range[26500, 26600]\n",
      "Working on missing range[26600, 26700]\n",
      "Working on missing range[26700, 26800]\n",
      "Working on missing range[26800, 26900]\n",
      "Working on missing range[26900, 27000]\n",
      "Working on missing range[27000, 27100]\n",
      "Working on missing range[27100, 27200]\n",
      "Working on missing range[27200, 27300]\n",
      "Working on missing range[27300, 27400]\n",
      "Working on missing range[27400, 27500]\n",
      "Working on missing range[27500, 27600]\n",
      "Working on missing range[27600, 27700]\n",
      "Working on missing range[27700, 27800]\n",
      "Working on missing range[27800, 27900]\n",
      "Working on missing range[27900, 28000]\n",
      "Working on missing range[28000, 28100]\n",
      "Working on missing range[28100, 28200]\n",
      "Working on missing range[28200, 28300]\n",
      "Working on missing range[28300, 28400]\n",
      "Working on missing range[28400, 28500]\n",
      "Working on missing range[28500, 28600]\n",
      "Working on missing range[28600, 28700]\n",
      "Working on missing range[28700, 28800]\n",
      "Working on missing range[28800, 28900]\n",
      "Working on missing range[28900, 29000]\n",
      "Working on missing range[29000, 29100]\n",
      "Working on missing range[29100, 29200]\n",
      "Working on missing range[29200, 29300]\n",
      "Working on missing range[29300, 29400]\n",
      "Working on missing range[29400, 29500]\n",
      "Working on missing range[29500, 29600]\n",
      "Working on missing range[29600, 29700]\n",
      "Working on missing range[29700, 29800]\n",
      "Working on missing range[29800, 29900]\n",
      "Working on missing range[29900, 30000]\n",
      "Working on missing range[30000, 30100]\n",
      "Working on missing range[30100, 30200]\n",
      "Working on missing range[30200, 30300]\n",
      "Working on missing range[30300, 30400]\n",
      "Working on missing range[30400, 30500]\n",
      "Working on missing range[30500, 30600]\n",
      "Working on missing range[30600, 30700]\n"
     ]
    }
   ],
   "source": [
    "# Checking 3 times for missing URIs... if they continue missing, it means they don't have an abstract. Most are due to being disambiguation pages\n",
    "# Example of missing URIs:\n",
    "# ['http://dbpedia.org/resource/Paul_Whelan', 'http://dbpedia.org/resource/Rovereto_(disambiguation)', 'http://dbpedia.org/resource/Irwin', 'http://dbpedia.org/resource/CEFCU', 'http://dbpedia.org/resource/Angolan_swallow', 'http://dbpedia.org/resource/Mountain_View,_New_Mexico', 'http://dbpedia.org/resource/Siege_of_Damascus', 'http://dbpedia.org/resource/Jerry_Simmons', 'http://dbpedia.org/resource/Frank_Richter', 'http://dbpedia.org/resource/CGU', 'http://dbpedia.org/resource/El_Gran_campeón', 'http://dbpedia.org/resource/Tsentaroy', 'http://dbpedia.org/resource/DoÃ±a_Ana_County,_New_Mexico', 'http://dbpedia.org/resource/Bethlehem_High_School', 'http://dbpedia.org/resource/List_of_unnumbered_minor_planets:_2003_A–R', 'http://dbpedia.org/resource/SPAP', 'http://dbpedia.org/resource/Charleston_Civic_Center', 'http://dbpedia.org/resource/ASAC', 'http://dbpedia.org/resource/Robert_Irwin', 'http://dbpedia.org/resource/Maarit_LepomÃ¤ki', 'http://dbpedia.org/resource/PloieÈ™ti', 'http://dbpedia.org/resource/Quattro_Canti,_Catania', 'http://dbpedia.org/resource/Peruvian_Brazilians', \"http://dbpedia.org/resource/Church_of_Sant'Angelo,_Perugia\", 'http://dbpedia.org/resource/Yelena_Vyalbe', 'http://dbpedia.org/resource/Saint-cyriens', 'http://dbpedia.org/resource/1994_Milan_Indoor_–_Singles', 'http://dbpedia.org/resource/Tuqtu', 'http://dbpedia.org/resource/Military_of_Mauritius', 'http://dbpedia.org/resource/Alistar_Fredericks', \"http://dbpedia.org/resource/2002_Kroger_St._Jude_International_–_Women's_Doubles\", 'http://dbpedia.org/resource/Wayne_Simmons', 'http://dbpedia.org/resource/Lindsay_(name)', 'http://dbpedia.org/resource/List_of_freshman_class_members_of_the_111th_United_States_Congress', 'http://dbpedia.org/resource/Neohumanism', 'http://dbpedia.org/resource/Kenan_IÅŸÄ±k', 'http://dbpedia.org/resource/Belfast_Duncairn', 'http://dbpedia.org/resource/Hum_Sab_Chor_Hain', 'http://dbpedia.org/resource/Flamengo_(disambiguation)', 'http://dbpedia.org/resource/Hà_Tĩnh_Province', 'http://dbpedia.org/resource/List_of_KDE_applications', 'http://dbpedia.org/resource/Sandusky_(surname)', 'http://dbpedia.org/resource/Kishibe', 'http://dbpedia.org/resource/Toyota_concept_vehicles_(1970–1979)', 'http://dbpedia.org/resource/FranÃ§ois_PÃ©ron', 'http://dbpedia.org/resource/WGCL-TV', \"http://dbpedia.org/resource/2017_Ricoh_Open_–_Women's_Doubles\", 'http://dbpedia.org/resource/KB_Financial_Group_Inc', 'http://dbpedia.org/resource/Arjun_Singh_(politician,_born_1962)', 'http://dbpedia.org/resource/103rd_Infantry_Division_Piacenza']\n",
    "\n",
    "check_for_missing_uri_descriptions_and_complete(uris_for_descriptions=uris_for_descriptions, step=100, prefix=\"missing_a_\")\n",
    "check_for_missing_uri_descriptions_and_complete(uris_for_descriptions=uris_for_descriptions, step=100, prefix=\"missing_b_\")\n",
    "check_for_missing_uri_descriptions_and_complete(uris_for_descriptions=uris_for_descriptions, step=100, prefix=\"missing_c_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have crawled as much as we could, we should remove the unfindable ones from our database\n",
    "def populate_candidate_description_dict():\n",
    "    all_cand_desc_results = load_cand_desc_results()\n",
    "\n",
    "    uris_for_descriptions_crawled = {}\n",
    "    for filename in all_cand_desc_results:\n",
    "        cands_desc = all_cand_desc_results[filename]\n",
    "        #print(list(cands_desc.keys())[0])\n",
    "        for key in cands_desc:\n",
    "            uris_for_descriptions_crawled[key] = cands_desc[key]\n",
    "    return uris_for_descriptions_crawled\n",
    "\n",
    "dict_candidate_description = populate_candidate_description_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dbpedia.org/resource/New_Amsterdam,_Guyana\n",
      "New Amsterdam (Dutch: Nieuw Amsterdam) is the regional capital of East Berbice-Corentyne, Guyana and one of the country's largest towns. It is 100 kilometres (62 mi) from the capital, Georgetown and located on the eastern bank of the Berbice River, 6 km (4 mi) upriver from its mouth at the Atlantic Ocean, and immediately south of the Canje River. New Amsterdam's population is 17,329 inhabitants as of 2012.\n"
     ]
    }
   ],
   "source": [
    "# Sanity check / testing by outputting a random candidate and its description\n",
    "cand_uri = list(dict_candidate_description.keys())[0]\n",
    "description = dict_candidate_description[cand_uri]\n",
    "\n",
    "print(cand_uri)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now data has been crawled appropriately. Next steps are to use an LLM with the description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs for evaluation:</br>\n",
    "* Grab descriptions for all URIs via SPARQL query\n",
    "* sameAs links</br>\n",
    "* is dbo:wikiPageRedirects of</br>\n",
    "        * Example: http://en.wikipedia.org/wiki/People's_Republic_of_China vs. http://dbpedia.org/resource/China</br>\n",
    "        * Relevant file: C:/Users/wf7467/Desktop/GitHub/KIT/agnostic-disambiguation/data/candidates/CoNLL_AIDA-YAGO2-dataset.nif/1.json</br>\n",
    "    * dbr:People's_Republic_Of_China</br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
